<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017) - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/logo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="研究成员推理攻击（Membership Inference Attack, MIA）：目标模型作为black box，要求判断某一条data record是否是用于训练该模型的数据。并探讨导致泄露的因素 本文主要针对有监督训练的模型，提出了shadow training的技术来为训练攻击模型构造数据集：首先，利用训练目标模型的API训练若干shadow models，模仿目标模型的行为接着，用训"><meta property="og:type" content="blog"><meta property="og:title" content="【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)"><meta property="og:url" content="http://example.com/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="研究成员推理攻击（Membership Inference Attack, MIA）：目标模型作为black box，要求判断某一条data record是否是用于训练该模型的数据。并探讨导致泄露的因素 本文主要针对有监督训练的模型，提出了shadow training的技术来为训练攻击模型构造数据集：首先，利用训练目标模型的API训练若干shadow models，模仿目标模型的行为接着，用训"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/gallery/covers/cover7.jpg"><meta property="article:published_time" content="2022-10-20T02:30:43.000Z"><meta property="article:modified_time" content="2022-10-24T02:01:08.555Z"><meta property="article:author" content="Aemilia Xu"><meta property="article:tag" content="Membership Inference Attack"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/gallery/covers/cover7.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/"},"headline":"【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)","image":["http://example.com/gallery/covers/cover7.jpg"],"datePublished":"2022-10-20T02:30:43.000Z","dateModified":"2022-10-24T02:01:08.555Z","author":{"@type":"Person","name":"Aemilia Xu"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"研究成员推理攻击（Membership Inference Attack, MIA）：目标模型作为black box，要求判断某一条data record是否是用于训练该模型的数据。并探讨导致泄露的因素 本文主要针对有监督训练的模型，提出了shadow training的技术来为训练攻击模型构造数据集：首先，利用训练目标模型的API训练若干shadow models，模仿目标模型的行为接着，用训"}</script><link rel="canonical" href="http://example.com/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/"><link rel="icon" href="/img/logo.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.loli.net/ajax/libs/pace/1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/gallery/covers/cover7.jpg" alt="【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-20T02:30:43.000Z" title="2022/10/20 10:30:43">2022-10-20</time>发表</span><span class="level-item"><time dateTime="2022-10-24T02:01:08.555Z" title="2022/10/24 10:01:08">2022-10-24</time>更新</span><span class="level-item">18 分钟读完 (大约2673个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)</h1><div class="content"><blockquote>
<p>研究成员推理攻击（Membership Inference Attack, MIA）：目标模型作为<strong>black box</strong>，要求判断某一条data record是否是用于训练该模型的数据。并探讨导致泄露的因素</p>
<p>本文主要针对有监督训练的模型，提出了shadow training的技术来为训练攻击模型构造数据集：<br>首先，利用训练目标模型的API训练若干shadow models，模仿目标模型的行为<br>接着，用训练好的shadow models来为attack model构造训练集数据、测试集数据（解决了目标模型是black box的问题，对预测向量的数据增强效果）<br>最终，attack model实现判断是否是训练集的二分类问题</p>
<p>攻击的实现主要是利用了目标模型在其训练集和非训练集上的输出行为差异，这一般由于训练时发生了过拟合导致，但其根本原因在于存在<strong>accuracy gap</strong>（模型在训练集和测试集上准确率的差异，对每个类别来说）</p>
</blockquote>
<span id="more"></span>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>目前<strong>商用</strong>的机器学习模型都是以a black-box API的形式来提供服务（例如根据用户喜好训练的推荐模型等等）。本实验通过训练一个<strong>攻击模型</strong>来实现成员推理攻击，通过区分目标模型对于训练数据和非训练数据的不同输出行为。</p>
<p>当模型是一个<strong>black box</strong>时，模型的结构、参数就是不可知的了（是white box时就是都知道的，之前2篇论文中都是以white box的形式来访问）</p>
<p>为了更好地实现攻击，引入了<strong>shadow training</strong>的技术：首先，创建多个模仿目标模型行为的“shadow models”，但这些模型的训练数据集是已知的，因此知道这些数据集中成员关系的基本真相。再用这些标记的输入和shadow models的输出来训练攻击模型</p>
<p>注意和<strong>model inversion</strong>的区别：model inversion是为了重建训练数据，或者从模型中得到某些class的属性、特征。而membership inference是为了判断某个数据是否是用于训练该模型的数据。也就是说，model inversion不关心单个数据record</p>
<h2 id="2-Machine-Learning-Background"><a href="#2-Machine-Learning-Background" class="headerlink" title="2 Machine Learning Background"></a>2 Machine Learning Background</h2><p>商用的训练模型由厂商选择，可能只基于自己的验证子集来设置模型的结构，那么在用户自己的数据集上训练时，很可能产生<strong>过拟合</strong>。即使现在有很多使用正则化来解决过拟合问题的方法，但是在商用模型中很少会控制正则化。</p>
<p>过拟合就会导致训练数据和非训练数据之间有明显差异</p>
<h2 id="3-Privacy-in-Machine-Learning"><a href="#3-Privacy-in-Machine-Learning" class="headerlink" title="3 Privacy in Machine Learning"></a>3 Privacy in Machine Learning</h2><blockquote>
<p>定义了研究的对象membership inference，强调了和model inversion之间的区别</p>
</blockquote>
<p>抵御membership inference attack是为了保护个体的隐私安全。而抵御model inversion是要保护一整个类的隐私信息（本文中认为model inversion很难抵御，例如某个疾病和某个基因有关，那么判断出有这种疾病就可以推断患者的这个基因，即class的特征是必然会被模型的判断结果所泄露的）</p>
<p>本文研究的membership inference attack是要评估一个人的membership risk，即若他的个人数据被用来训练一个模型（如推荐系统），是否会泄露其个人信息</p>
<h2 id="4-Problem-Statement"><a href="#4-Problem-Statement" class="headerlink" title="4 Problem Statement"></a>4 Problem Statement</h2><blockquote>
<p>描述了研究的问题，前提假设条件</p>
</blockquote>
<p>针对<strong>分类</strong>模型</p>
<p>假设敌手可以获得输出的prediction vector（在每一个类别上的打分）；知道输入输出的形式、取值范围、知道模型的结构、算法或者可以访问<u>训练模型的<strong>learning oracle</strong></u>（二选一，后者不知道具体的参数、结构）；知道训练集分布的背景知识（如有相同的分布的不相交子集、或者知道其边际分布</p>
<p>批判标准：<strong>precision</strong>（精确率，判断为真的中有多少是对的）和<strong>recall</strong>（召回率，多少真的被判断为真的了）</p>
<h2 id="5-Membership-Inference"><a href="#5-Membership-Inference" class="headerlink" title="5 Membership Inference"></a>5 Membership Inference</h2><blockquote>
<p>具体的攻击方法</p>
</blockquote>
<h3 id="A-Overview-of-the-attack"><a href="#A-Overview-of-the-attack" class="headerlink" title="A. Overview of the attack"></a>A. Overview of the attack</h3><p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221018153609638.png" style="zoom:50%;"></p>
<h3 id="B-Shadow-models"><a href="#B-Shadow-models" class="headerlink" title="B. Shadow models"></a>B. Shadow models</h3><p>攻击者要创建$<br>k<br>$个shadow model $f^{i}_{shadow}()$，训练的数据集为$D^{train}_{shadow^i}$（和目标模型的数据集的分布相似，但是不相交。虽然在商业模式下的目标模型的训练算法和结构不可知，但是可以同样利用这个商业服务来训练shadow model（如下图所示，其中$train()$就是服务商提高的API）</p>
<p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221018161620256.png" style="zoom:33%;"></p>
<p>shadow model越多，攻击模型的准确度就越高。那么接下去的问题就是用于训练shadow model的数据$D^{train}_{shadow^i}$如何而来</p>
<h3 id="C-Generating-training-data-for-shadow-models"><a href="#C-Generating-training-data-for-shadow-models" class="headerlink" title="C. Generating training data for shadow models"></a>C. Generating training data for shadow models</h3><blockquote>
<p>介绍了几种生成目标模型训练集分布接近的数据集的方法</p>
</blockquote>
<ol>
<li><p>Model-based synthesis（使用目标模型来构造数据集）</p>
<p>分为2个阶段</p>
<p>（1）用hill-climbing算法$search$可能数据的空间，即随机生成$x$，选择其中得分高的（针对具体的某个类c）</p>
<p>（2）从上述数据集中$sample$合成的数据</p>
<p>在$search$（迭代过程）时，设置了$j$来记录失败次数，$k$用来控制围绕被接受的记录的搜索直径，以便提出新的记录（每次提出新的x时，只替换k个特征，即搜索半径）</p>
<p>当$x$的评分向量$y$中$y_c$最大（表明这个$x$是$c$类的），而且$y_c$超过了预先设定的指标$conf_{min}$，那么就选择这个$x$</p>
<p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221018164911656.png" style="zoom:50%;"></p>
</li>
<li><p>Statistic-based synthesis</p>
<p>若敌手知道训练集的统计信息，那么直接从这个分布中合成数据即可</p>
</li>
<li><p>Noisy real data</p>
<p>若敌手可访问和训练集类似的数据集，然后随机反转其中10%或20%的特征，就可以作为训练shadow models的数据集了</p>
</li>
</ol>
<h3 id="D-Training-the-attack-model"><a href="#D-Training-the-attack-model" class="headerlink" title="D. Training the attack model"></a>D. Training the attack model</h3><p>主要的思想就是在相似的训练集上以相同方式训练的模型，其行为是相似的</p>
<p>给shadow models一系列输入，得到其相应的输出，若是shadow model对应的训练集中的数据，就标记为$in$，若不是，则标记为$out$。这些记录用于训练attack model，并分成$c_{target}$个划分，分别对应目标模型的$c_{target}$个类别。</p>
<p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221018170041333.png" style="zoom:80%;"></p>
<h2 id="问询shadow-models时用的测试集和训练集是不相交的（否则有歧义）"><a href="#问询shadow-models时用的测试集和训练集是不相交的（否则有歧义）" class="headerlink" title="问询shadow models时用的测试集和训练集是不相交的（否则有歧义）"></a>问询shadow models时用的测试集和训练集是不相交的（否则有歧义）</h2><h2 id="这里训练的attack-model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）"><a href="#这里训练的attack-model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）" class="headerlink" title="这里训练的attack model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）"></a>这里训练的attack model要解决的是<strong>二分类问题</strong>，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）</h2><p>（感觉思想挺简单的，但是要实现比较复杂，有好多shadow models，而且训练shadow models时的数据集很重要）</p>
<h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6 Evaluation"></a>6 Evaluation</h2><blockquote>
<p>介绍实验中使用的数据集、目标模型，展示实验结果</p>
</blockquote>
<p>实验结果表明：本文所提出的攻击方法的鲁棒性很好，即使对目标模型的训练集的分布猜测有误，攻击的准确性还是很高。</p>
<p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221019163532752.png" style="zoom:50%;"></p>
<p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221019163732132.png" style="zoom:50%;"></p>
<p>【!】当类的训练数据在整个训练集的比重较小时，攻击在这些类上的精确度（precision）上较低。</p>
<h3 id="7-类的数量，和每个类的训练数据集对攻击的影响"><a href="#7-类的数量，和每个类的训练数据集对攻击的影响" class="headerlink" title="7 类的数量，和每个类的训练数据集对攻击的影响"></a>7 类的数量，和每个类的训练数据集对攻击的影响</h3><p>类的数量越多，泄露的模型的内部状态越多。因为当数据集中类越多，目标模型就需要提取出更多特征、“记住”更多数据的信息来准确判断类别，所以也就会泄露更多信息。</p>
<p><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/image-20221019164536491.png" style="zoom:50%;"></p>
<h3 id="【？？？】"><a href="#【？？？】" class="headerlink" title="【？？？】"></a>【？？？】</h3><p>为什么这里说</p>
<p>“the more data in the training dataset is associated with a given class, the lower the attack precision for that class”</p>
<p>但是之前上面说“The reason for the attack’s low precision on some classes is that the target classifier cannot confidently model the distribution of data records belonging to these classes—because it has not seen enough examples.”</p>
<p>感觉有点矛盾？</p>
<h3 id="过拟合对攻击的影响"><a href="#过拟合对攻击的影响" class="headerlink" title="过拟合对攻击的影响"></a>过拟合对攻击的影响</h3><p>同一种类的模型中，<strong>过拟合程度</strong>越大，模型泄露的信息越多。但是相同过拟合程度下，不同模型的信息泄露程度不一样。【!】所以，过拟合不是导致易受Membership Inference Attack的唯一原因。</p>
<p>模型结构中很重要的因素是<strong>accuracy gap</strong>（模型在训练集和测试集上准确率的差异，对每个类别来说）。当一个类别在某个模型中的accuracy gap越大，攻击在这类上的精确度（precision）越高。</p>
<p>毕竟MIA就是要区分目标模型在测试集和训练集上的行为差异，那么当accuracy gap越大，就说明二者的差异越大，攻击自然就越容易成功了，。</p>
<h2 id="8-MIA成功的原因与抵御"><a href="#8-MIA成功的原因与抵御" class="headerlink" title="8 MIA成功的原因与抵御"></a>8 MIA成功的原因与抵御</h2><p>MIA的成功与否与目标模型的普适性（generalizability）&amp;训练集数据的多样性有关。</p>
<p>因为过拟合会显著导致易受MIA攻击，所以解决过拟合问题的方法是可以用来抵御MIA的。常见的方法由<strong>dropout</strong>（）、<strong>Regularization</strong>（正则化）。</p>
<p>从训练过程角度，使用差异化的私有模型（differentially private models，就是和差分隐私有关的），这样一个模型是包含一条数据训练而得的概率和包含这条数据而得的概率相近，这样也可以用来抵御MIA。</p>
<p>从这些商用模型训练API的服务商角度，应该告知用户过拟合的风险、为数据集提供更加适合的模型结构等等。</p>
<h3 id="A-Mitigation-strategies"><a href="#A-Mitigation-strategies" class="headerlink" title="A. Mitigation strategies"></a>A. Mitigation strategies</h3><p>几种抵御MIA的策略如下</p>
<ul>
<li>Restrict the prediction vector to top k classes</li>
</ul>
<ul>
<li>Coarsen precision of the prediction vector</li>
</ul>
<ul>
<li>Increase entropy of the prediction vector</li>
</ul>
<ul>
<li>Use regularization</li>
</ul>
<p>根据作者的实验结果，只有正则化比较有用。可见在训练模型时，正则化非常重要！！</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)</p><p><a href="http://example.com/2022/10/20/论文笔记3/">http://example.com/2022/10/20/论文笔记3/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Aemilia Xu</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2022-10-20</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-10-24</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Membership-Inference-Attack/">Membership Inference Attack</a></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/css/share.min.css"><div class="social-share"></div><script src="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">【论文笔记】Data-Free Model Extraction（CVPR 2021）</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02/"><span class="level-item">【论文笔记】Plug &amp; Play Attacks—— Towards Robust and Flexible Model Inversion Attacks（ICMP 2022）</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="valine-thread"></div><script src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/valine/1.4.16/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread',
            appId: "zbTLXHHTVr7ZTCgT51zCUL5d-9Nh9j0Va",
            appKey: "YsG6Msj1LDpajrlzeeHjHUdE",
            
            avatar: "mm",
            avatarForce: false,
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "zh-CN",
            visitor: true,
            highlight: true,
            recordIP: false,
            
            
            
            enableQQ: false,
            requiredFields: [],
        });</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-3-desktop is-3-widescreen  order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#1-Introduction"><span class="level-left"><span class="level-item">1 Introduction</span></span></a></li><li><a class="level is-mobile" href="#2-Machine-Learning-Background"><span class="level-left"><span class="level-item">2 Machine Learning Background</span></span></a></li><li><a class="level is-mobile" href="#3-Privacy-in-Machine-Learning"><span class="level-left"><span class="level-item">3 Privacy in Machine Learning</span></span></a></li><li><a class="level is-mobile" href="#4-Problem-Statement"><span class="level-left"><span class="level-item">4 Problem Statement</span></span></a></li><li><a class="level is-mobile" href="#5-Membership-Inference"><span class="level-left"><span class="level-item">5 Membership Inference</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-Overview-of-the-attack"><span class="level-left"><span class="level-item">A. Overview of the attack</span></span></a></li><li><a class="level is-mobile" href="#B-Shadow-models"><span class="level-left"><span class="level-item">B. Shadow models</span></span></a></li><li><a class="level is-mobile" href="#C-Generating-training-data-for-shadow-models"><span class="level-left"><span class="level-item">C. Generating training data for shadow models</span></span></a></li><li><a class="level is-mobile" href="#D-Training-the-attack-model"><span class="level-left"><span class="level-item">D. Training the attack model</span></span></a></li></ul></li><li><a class="level is-mobile" href="#问询shadow-models时用的测试集和训练集是不相交的（否则有歧义）"><span class="level-left"><span class="level-item">问询shadow models时用的测试集和训练集是不相交的（否则有歧义）</span></span></a></li><li><a class="level is-mobile" href="#这里训练的attack-model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）"><span class="level-left"><span class="level-item">这里训练的attack model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）</span></span></a></li><li><a class="level is-mobile" href="#6-Evaluation"><span class="level-left"><span class="level-item">6 Evaluation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#7-类的数量，和每个类的训练数据集对攻击的影响"><span class="level-left"><span class="level-item">7 类的数量，和每个类的训练数据集对攻击的影响</span></span></a></li><li><a class="level is-mobile" href="#【？？？】"><span class="level-left"><span class="level-item">【？？？】</span></span></a></li><li><a class="level is-mobile" href="#过拟合对攻击的影响"><span class="level-left"><span class="level-item">过拟合对攻击的影响</span></span></a></li></ul></li><li><a class="level is-mobile" href="#8-MIA成功的原因与抵御"><span class="level-left"><span class="level-item">8 MIA成功的原因与抵御</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-Mitigation-strategies"><span class="level-left"><span class="level-item">A. Mitigation strategies</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2023 Aemilia Xu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>