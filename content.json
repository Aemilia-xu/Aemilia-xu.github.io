{"posts":[{"title":"【论文笔记】Plug &amp; Play Attacks—— Towards Robust and Flexible Model Inversion Attacks（ICMP 2022）","text":"https://zhuanlan.zhihu.com/p/536091331 摘要关于Model Inversion Attacks（模型反转攻击），即通过利用模型的learned knowledge，从目标分类器的私有训练数据中创造出反映class特征的合成图像。相当于从模型中提取出每种类别特定的特征。这也是很严重安全隐患，因为可能可以提取出如人脸、指纹、身份信息等敏感隐私信息，那么攻击者可以重构人脸，冒用身份…… 以前的研究都是训练GAN（生成对抗网络）来作为image priors（先验）。但是存在耗时、耗力、易受数据集分布变化的影响。本文提出的Plug &amp; Play攻击可以减少对image prior的依赖，只需要一个GAN网络即可对大范围的目标进行攻击。而且即使使用的是预训练好的GAN模型、数据分布发生很大变化也可以达到很好的攻击效果。 1 Introduction为了解决训练过程中会出现的梯度消失的问题，使用Poincare损失函数，而不是交叉熵损失函数。加入随机转化避免过拟合。首次提出了要从攻击结果中找出有意义的样本的重要性。 2 Model Inversion in Deep Learning 介绍了以前一些攻击方法，及其缺陷。 目前有三种攻击方式：optimization-based, training-based, or architecture-based optimization-based：通过产生合成的模型输入来找类型特征，也基于梯度下降的方法 training-based：训练GAN模型，将目标模型视作一个加密器，要找解密器 这些都不是为了获取隐私信息，只是探寻目标模型各个类别的特征 可能存在fooling image（无意义的噪点图像），out-of-distribution data（如猫狗分类中出现了一个熊猫图像） 影响因素：distributional shifts, vanishing gradients, and non-robust target models 3 Generative Model Inversion Attacks 定义了理想的MIA攻击，以及会影响MIA攻击效果的因素 假设：目标分类模型为，且为对x是否为类c∈C的预测分数，敌手可以访问目标模型（作为white box），且无限次地问询，但并不知道C的任何信息。 敌手要构造一个合成图像x^，满足时目标类别c的特征，泄露了隐私信息。这在较浅的神经网络中可行，但是现在流行的深层神经网络就不行了。 之后产生了使用GAN网络来生成样本（将特征向量z映射到图像空间），并训练鉴别器D来鉴别是否是真实样本x还是由G生成的G(z)，得到等式。这样或许可以得到评分（$y_c=M_{target}(\\overset{\\thicksim}{x} )_c$）最高的$ \\overset{\\thicksim}{x} = G(\\overset{\\thicksim}{z})$。但是这个构造得到的$\\overset{\\thicksim}{x}$不一定具有意义。 为图像的分布，令为X中的样本可能具有的人类可识别特征的分布，令为c类的特征。如再面部图像中，F和P的区别在于，F可能包含发色、皱纹、瞳孔间距等面部特征，而P还多了与身份信息无关的（如服装、背景等）。也就是说，$F$只包含和人物的身份信息有关的信息，而$P$还包含了无用的信息。 注意，不同类别之间的特征可能相同。 现假设，有2个类别满足，且，即特征不相同。 G可以拟合P(X)，进而近似F(X)，然后合成符合这个分布的样本x。 【问题】$\\overset{\\thicksim}{X}$是如何得到的 4 Towards Robust and Flexible MIAs 介绍本文提出的Plug &amp; Play攻击方式。这种方式鲁棒性更好，也更适用于distributional shift setting分布转换设置 首先，latent vectors潜伏向量被采样并映射到它们的中间表征w（用预训练的StyleGAN2模型） 然后根据w生成图像，进行转换（Transformations），并输入目标模型。 最后，在目标模型的输出和目标类别c上计算Poincare损失，并通过反向传播损失和执行梯度下降步骤更新w。优化完成后，根据其对随机变换的鲁棒性，选择一个结果子集。 4.1. Target-Independent Image Priors采用预训练的StyleGAN2模型作为图像的先验（不需要辅助输入和训练特定数据集的模型）。只需要是同一领域预训练好的模型。 这里预训练好的StyleGAN2包含2个模块：会将随机的latent vector潜伏向量（服从标准正态分布）的z映射到intermediate latent representation中间潜伏向量w。则根据w来生成图片 4.2. Increasing Robustness by Transformations进行一系列的图像变换，并令。在优化过程中，先进行这些可导的图像变换，再输入到目标模型中得到预测分数。即在前向传播过程中计算的是 这样，若这些变换后的图像越接近目标分布，攻击就会越成功。而且可以增加生成的图像的鲁棒性 4.3 Overcoming Vanishing Gradients以前的MIA都基于交叉熵损失函数，这样关于output logit的偏导为，容易发生梯度消失。而初始的潜伏向量又是随机产生的，随着调整，其预测分数会越来越高，导致梯度趋于消失。这样如果预先的采样不好的话，由于梯度下降的问题，对初始图像的改动就很小，攻击就会不成功。 为了解决这个问题，采用Poincaré distance作为损失函数。（其中$||·||_2$是欧几里得范数）。它是一个特殊hyperbolic space(双曲空间)中的距离度量。在这个空间里面，一个圆的面积随着半径的增加指数型增加。 【注】欧几里得范数即$L2$范数 4.4 Selecting Meaningful Attack Results 选用的方法是transformation-based selection，即通过变换，重新计算得分，选择最高的 攻击结果可能存在误导。本文采取的方法如下： 选取大量的符合正态分布的样本，并映射到intermediate latent space中间隐藏空间$W$ 为每一个w生成图像，并进行剪裁和调整大小的变换。计算平均预测分数，以及horizontally flipped counterpart with $M_{target}$，为每个类别c选择其中得分最高的初始图像。 根据蒙特卡罗方法 计算预期稳健预测分数，并进行$N=100$次随机的图像变换，选取其中得分最高的50个作为最终攻击结果。【注意】这里选取的变换要和优化过程中选取的不同或更强（否则poorly generated的样本可能会过拟合） 总结Plug &amp; Play攻击要解决的问题就是： 5 Experiments 介绍了验证Plug &amp; Play有效性的实验，和其他攻击方法的对比 评价标准有： 在目标模型的数据集上训练Inception-v3模型，然后用该模型对攻击结果进行打分，给出在目标类别上的top-1和top-5准确率。 计算平均特征距离$δ_{eval}$，针对面部图像用预训练的FaceNet来测量特征距离$δ_{face}$，越小说明越接近训练数据。 Fréchet inception distance（FID）","link":"/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment aaaaa","link":"/2022/10/18/hello-world/"},{"title":"【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)","text":"研究成员推理攻击（Membership Inference Attack, MIA）：目标模型作为black box，要求判断某一条data record是否是用于训练该模型的数据。并探讨导致泄露的因素 本文主要针对有监督训练的模型，提出了shadow training的技术来为训练攻击模型构造数据集：首先，利用训练目标模型的API训练若干shadow models，模仿目标模型的行为接着，用训练好的shadow models来为attack model构造训练集数据、测试集数据（解决了目标模型是black box的问题，对预测向量的数据增强效果）最终，attack model实现判断是否是训练集的二分类问题 攻击的实现主要是利用了目标模型在其训练集和非训练集上的输出行为差异，这一般由于训练时发生了过拟合导致，但其根本原因在于存在accuracy gap（模型在训练集和测试集上准确率的差异，对每个类别来说） 1 Introduction目前商用的机器学习模型都是以a black-box API的形式来提供服务（例如根据用户喜好训练的推荐模型等等）。本实验通过训练一个攻击模型来实现成员推理攻击，通过区分目标模型对于训练数据和非训练数据的不同输出行为。 当模型是一个black box时，模型的结构、参数就是不可知的了（是white box时就是都知道的，之前2篇论文中都是以white box的形式来访问） 为了更好地实现攻击，引入了shadow training的技术：首先，创建多个模仿目标模型行为的“shadow models”，但这些模型的训练数据集是已知的，因此知道这些数据集中成员关系的基本真相。再用这些标记的输入和shadow models的输出来训练攻击模型 注意和model inversion的区别：model inversion是为了重建训练数据，或者从模型中得到某些class的属性、特征。而membership inference是为了判断某个数据是否是用于训练该模型的数据。也就是说，model inversion不关心单个数据record 2 Machine Learning Background商用的训练模型由厂商选择，可能只基于自己的验证子集来设置模型的结构，那么在用户自己的数据集上训练时，很可能产生过拟合。即使现在有很多使用正则化来解决过拟合问题的方法，但是在商用模型中很少会控制正则化。 过拟合就会导致训练数据和非训练数据之间有明显差异 3 Privacy in Machine Learning 定义了研究的对象membership inference，强调了和model inversion之间的区别 抵御membership inference attack是为了保护个体的隐私安全。而抵御model inversion是要保护一整个类的隐私信息（本文中认为model inversion很难抵御，例如某个疾病和某个基因有关，那么判断出有这种疾病就可以推断患者的这个基因，即class的特征是必然会被模型的判断结果所泄露的） 本文研究的membership inference attack是要评估一个人的membership risk，即若他的个人数据被用来训练一个模型（如推荐系统），是否会泄露其个人信息 4 Problem Statement 描述了研究的问题，前提假设条件 针对分类模型 假设敌手可以获得输出的prediction vector（在每一个类别上的打分）；知道输入输出的形式、取值范围、知道模型的结构、算法或者可以访问训练模型的learning oracle（二选一，后者不知道具体的参数、结构）；知道训练集分布的背景知识（如有相同的分布的不相交子集、或者知道其边际分布 批判标准：precision（精确率，判断为真的中有多少是对的）和recall（召回率，多少真的被判断为真的了） 5 Membership Inference 具体的攻击方法 A. Overview of the attack B. Shadow models攻击者要创建$k$个shadow model $f^{i}_{shadow}()$，训练的数据集为$D^{train}_{shadow^i}$（和目标模型的数据集的分布相似，但是不相交。虽然在商业模式下的目标模型的训练算法和结构不可知，但是可以同样利用这个商业服务来训练shadow model（如下图所示，其中$train()$就是服务商提高的API） shadow model越多，攻击模型的准确度就越高。那么接下去的问题就是用于训练shadow model的数据$D^{train}_{shadow^i}$如何而来 C. Generating training data for shadow models 介绍了几种生成目标模型训练集分布接近的数据集的方法 Model-based synthesis（使用目标模型来构造数据集） 分为2个阶段 （1）用hill-climbing算法$search$可能数据的空间，即随机生成$x$，选择其中得分高的（针对具体的某个类c） （2）从上述数据集中$sample$合成的数据 在$search$（迭代过程）时，设置了$j$来记录失败次数，$k$用来控制围绕被接受的记录的搜索直径，以便提出新的记录（每次提出新的x时，只替换k个特征，即搜索半径） 当$x$的评分向量$y$中$y_c$最大（表明这个$x$是$c$类的），而且$y_c$超过了预先设定的指标$conf_{min}$，那么就选择这个$x$ Statistic-based synthesis 若敌手知道训练集的统计信息，那么直接从这个分布中合成数据即可 Noisy real data 若敌手可访问和训练集类似的数据集，然后随机反转其中10%或20%的特征，就可以作为训练shadow models的数据集了 D. Training the attack model主要的思想就是在相似的训练集上以相同方式训练的模型，其行为是相似的 给shadow models一系列输入，得到其相应的输出，若是shadow model对应的训练集中的数据，就标记为$in$，若不是，则标记为$out$。这些记录用于训练attack model，并分成$c_{target}$个划分，分别对应目标模型的$c_{target}$个类别。 问询shadow models时用的测试集和训练集是不相交的（否则有歧义）这里训练的attack model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）（感觉思想挺简单的，但是要实现比较复杂，有好多shadow models，而且训练shadow models时的数据集很重要） 6 Evaluation 介绍实验中使用的数据集、目标模型，展示实验结果 实验结果表明：本文所提出的攻击方法的鲁棒性很好，即使对目标模型的训练集的分布猜测有误，攻击的准确性还是很高。 【!】当类的训练数据在整个训练集的比重较小时，攻击在这些类上的精确度（precision）上较低。 7 类的数量，和每个类的训练数据集对攻击的影响类的数量越多，泄露的模型的内部状态越多。因为当数据集中类越多，目标模型就需要提取出更多特征、“记住”更多数据的信息来准确判断类别，所以也就会泄露更多信息。 【？？？】为什么这里说 “the more data in the training dataset is associated with a given class, the lower the attack precision for that class” 但是之前上面说“The reason for the attack’s low precision on some classes is that the target classifier cannot confidently model the distribution of data records belonging to these classes—because it has not seen enough examples.” 感觉有点矛盾？ 过拟合对攻击的影响同一种类的模型中，过拟合程度越大，模型泄露的信息越多。但是相同过拟合程度下，不同模型的信息泄露程度不一样。【!】所以，过拟合不是导致易受Membership Inference Attack的唯一原因。 模型结构中很重要的因素是accuracy gap（模型在训练集和测试集上准确率的差异，对每个类别来说）。当一个类别在某个模型中的accuracy gap越大，攻击在这类上的精确度（precision）越高。 毕竟MIA就是要区分目标模型在测试集和训练集上的行为差异，那么当accuracy gap越大，就说明二者的差异越大，攻击自然就越容易成功了，。 8 MIA成功的原因与抵御MIA的成功与否与目标模型的普适性（generalizability）&amp;训练集数据的多样性有关。 因为过拟合会显著导致易受MIA攻击，所以解决过拟合问题的方法是可以用来抵御MIA的。常见的方法由dropout（）、Regularization（正则化）。 从训练过程角度，使用差异化的私有模型（differentially private models，就是和差分隐私有关的），这样一个模型是包含一条数据训练而得的概率和包含这条数据而得的概率相近，这样也可以用来抵御MIA。 从这些商用模型训练API的服务商角度，应该告知用户过拟合的风险、为数据集提供更加适合的模型结构等等。 A. Mitigation strategies几种抵御MIA的策略如下 Restrict the prediction vector to top k classes Coarsen precision of the prediction vector Increase entropy of the prediction vector Use regularization 根据作者的实验结果，只有正则化比较有用。可见在训练模型时，正则化非常重要！！","link":"/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/"},{"title":"【论文笔记】Reconstructing Training Data with Informed Adversaries（S&amp;P 2022）","text":"摘要&amp;介绍由于机器学习模型可以有“记忆”功能，那么当训练数据涉及隐私信息时，若被攻击者还原出原始训练数据，就会很危险。这个论文就是探讨这样的攻击是否可能。 所有的训练数据中，只有一个是未知的。目标就是还原出这一个未知的数据样本，找到攻击的方式、泄露多少信息可以达到被还原、哪些特性会导致可还原、自我检查模型的安全性（不泄露信息） 本文提出了一种研究ML模型重构攻击的可行性的通用方法，而无需假设模型的类型或访问中间梯度，并启动了一项能够防止这类攻击的缓解策略的研究 RecoNN一种用于重现数据而训练的神经网络 II. Reconstruction with Informed Adversaries该部分主要介绍了为什么定义了这样很强的infromed adversaries，即敌手知道除了target以外的所有数据。以及这样的reconstruction攻击和membership inference攻击、attribute inference攻击之间的关系。 III. Reconstruction in convex settingA. Reconstruction Strategy for Convex Models针对ERM（经验风险最小化问题）下凸监督学习模型的攻击，在没有side knowledge的时候，对线性回归、岭回归、Logistic回归都可以实现攻击。 若风险函数为，且，则最优解为 敌手有访问模型的white box（说明知道模型的结构、参数信息），并且有其他所有样本D_ 的信息，那么target z就满足一个方程式 监督学习中一个样本用(x,y)即d维的特征向量x和1维的标签y来表示，则要从d’个等式中解出d+1个未知数（p.s. d’是模型空间的维度） 【解读】为什么是d’个等式？因为模型空间中，每个模型都有一个方程 B. Closed-Form Reconstruction Against GLMs 证明在用截距项拟合的GLM（广义线性模型）的情况下，存在这种攻击的闭式解决方案。 这个攻击下，informed敌手不需要其他关于z的side knowledge 【证明】 IV. A General Reconstruction Attack 针对一般的机器学习(ML)模型的重建攻击 直观上，$z$对模型$θ=A(D_∪{z})$的影响和$\\overset{\\thicksim}{z}$对$\\overset{\\thicksim}{θ}=A(D_ ∪\\overset{\\thicksim}{z})$的影响是一样的，所以可以通过反复尝试来找到$z$ A. General Attack Strategy记$A_{D_}=A(D_ ∪{z})$，那么当$D_$ 给定时，A就是Z→Θ的函数。那么reconstruction攻击就是要解出$z=A_{D_}^{-1}(θ)$ 【注意】这里是一般的ML模型，所以不一定能够保证是convex的，而且模型训练过程中可能有随机性 因为假设了敌手特别强大，所以可以采用枚举的方式，找到最接近θ的论文中采用的机制是using “neural networks to attack neural networks”，（用魔法打败魔法hhhh），构建了RecoNN模型 B. Training Reconstructor Networks本文中重构z的步骤如下 $\\bar D=\\{\\bar z_1,\\bar z_2,…,\\bar z_k\\}$是$Z$中的shadow targets，这属于关于$z$的side knowledge 枚举$\\bar D$中的所有元素，得到$\\bar θ_i$，从而得到了attack training data攻击训练数据 使用训练算法R得到RecoNN模型$Φ$ 应用$Φ$，得到结果$\\bar z$ 此外，假设了$X$和$Y$都是有限的，且$y$可以从$x$中直接推断出，那么只要重构样本的特征向量$x$即可 具体的训练方式见后面的讨论 using “neural networks to attack neural networks”的相关工作被应用于了membership inference，model inversion模型反演，property inference V. Experimental SetupA. Default Settings实验中使用的模型的超参数 a. 数据集的分割：$D_$、D^和测试目标集是三个不相交的集合 b. 生成模型的训练：训练方式采用的就是标准的梯度下降法，且使用了full batches。假设了敌手直到模型的初始状态，所有模型的初始状态都一致（之后也有讨论使用mini-batching和随机初始化情况下的攻击） MINST数据集上训练出来的模型准确率更加高（因为CIFAR-10更加复杂），所以对CIFAR-10的攻击可能更加困难，需要更加庞大的shadow points去训练RecoNN c. 重建模型的训练：When training the reconstructor, shadow model parameters across layers are flattened and concatenated together.？？？什么意思. 在训练的时候，跨层的参数被扁平化，且连接在一起。还需要把所有的坐标点都缩放到0均值和平均方差，因为有些参数很小。 针对MINST，采用MAE（平均绝对误差）+MSE（均方误差）的方式来计算shadow targets和reconstructor outputs之间的loss。 针对CIFAR-10，增加了LPIPS loss（感知损失）和GAN-like Discriminator loss，都是为了提高图像的视觉质量 B. Criteria for Attack Success介绍了实验中设置的评估指标 a. MSE：均方差 b. LPIPS：感知损失，即比较深度特征，更加接近人类视觉 c. KL散度： d. Nearest Neighbor Oracle: 一个oracle，可以猜测离z最接近的点z^ VI. Empirical Studies in reconstruction总结采用的策略是针对已知的所有训练数据和shadow targets训练得到的所有模型θ~i~来训练RecoNN模型Φ，来得到θ对应的样本z，即using “neural networks to attack neural networks” 相关知识 membership inference attacks (MIA)：成员推理攻击，即判断一个对象是否是模型训练时使用的数据https://blog.csdn.net/xff1994/article/details/89964552 attribute inference attacks (AIA) ：推理攻击 property inference attacks (PIA)：属性推理攻击，AIA的一种泛化攻击，一般都是得到样本数据的一些属性、平均信息，不会损害个人隐私 GLM广义线性模型：https://zhuanlan.zhihu.com/p/22876460 https://xg1990.com/blog/archives/304 例如线性回归中假设y|x服从正态分布，Logistic回归中假设y|x服从二项分布 ![](论文笔记1/image-20221008144438312.png) canonical exponential family 指数分布族 而在GLM中，假设y|x满足这种指数分布族是因为这样下的y|x的期望、方差都非常简单，而要求的目标函数h(x;θ)就是y|x的期望，这样就很简单了。 canonical link function 正则关联函数，是canonical response function（正则响应函数）的倒数 LPIPS loss（感知损失）：用深度特征来度量图像的相似度 https://zhuanlan.zhihu.com/p/206470186 GAN-like Discriminator loss GAN网络（生成对抗网络是要通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布） 问题 没明白”When training the reconstructor, shadow model parameters across layers are flattened and concatenated together.“ 扁平化就是说降低维度吧。","link":"/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01/"},{"title":"【论文笔记】Data-Free Model Extraction（CVPR 2021）","text":"（还不是很完善orz） 本文证明并解决了现有model extraction对数据集的依赖（依赖于替代数据集与实际训练集的相似性），并且在目标模型以black box形式被访问的条件下，提出了data-free的model extraction攻击，称为data-free model extraction (DFME)。DFME中采用了data-free knowledge transfer中的一些技术。 利用GAN网络来训练攻击模型，其中一个generator负责生成训练数据，另一个student model来学习目标模型在这些数据上的行为（做到data free） 使用$l_1$范式损失函数（防止梯度消失） 使用零阶优化（前向差分法）来逼近梯度（解决目标模型是个black box，无法获知其真实梯度的问题） 从概率分布中还原目标模型的logits 1 相关工作1.1 Data-Free Knowledge Distillation（无数据知识蒸馏）可参考https://zhuanlan.zhihu.com/p/516930757、https://zhuanlan.zhihu.com/p/102038521（知识蒸馏） 知识蒸馏是一种模型压缩方法，是一种基于“teacher-student网络思想”的训练方法，目标是把一个训练好的模型（teacher，Net-T）的知识压缩给另外一个模型（student，Net-S），达到节省模型训练的开销的作用。其中，Net-T一般结构复杂、参数量大，而Net-S结构较为简单、参数量小。那么在用Net-T来训练Net-S的时候，就是让Net-S去学习Net-T的泛化能力，直接的办法是使用softmax层输出的类别的概率来作为“soft target”。 soft target相对应的是hard target。一般的标签都是hard target，就是说一个对象只有一个标签。而soft target则是在每个标签上都有一定的值。举例来说，一个写的像3的2，在”3”上具有一定的概率值。而写的像7的2，在”7”上有一定的概率值，那么他们就提供了更多的信息。 【参考】https://antkillerfarm.github.io/dl%20acceleration/2019/07/27/DL_acceleration_6.html 使用soft target来训练Net-S时，softmax函数就变成了如下的形式 （具体先不展开，看看之后需要用到哪些KD相关的知识再说） 1.2 Generative Models本文中使用GAN类似的方法去训练student，用GAN去拟合最能说明当前student和teacher的决策曲面之间差异的数据分布（好复杂。。。） 1.3 Black-box Gradient Approximation因为目标模型是个black box，无法获得其梯度，所以采用的是零阶优化的方式来做到梯度逼近（之后会再介绍） 2 How Hard is it to Find a Surrogate Dataset?实验证明了现有model extraction都是基于替代数据集（surrogate dataset）非常接近与原始训练集的，若是替代集和原始训练集差别比较大，攻击的准确率就会低。 但是这样的surrogate dataset在现实中难以获得，于是作者提出了DFME 3 Data-Free Model Extraction 介绍了本文提出的攻击方法 DFME的目标是使extraction model对x的预测和目标模型对x的预测不同的概率最小。其中$D_V$是目标模型的训练集（攻击者不可知），所以可以用合成的数据集$D_S$ 训练extraction model时，优化时使用的损失函数是$L$，则要满足 3.1 Overview GFME的过程就是一个GAN网络，generator $G$生成输入$x$，目标网络$V$和Student网络$S$作为discriminator，$S$要去学习$V$对输入$x$的预测行为。其中$z$是一个随机的噪音，$G$通过给到的$z$来生成输入$x$。然后通过反向传播来优化。但是目标模型$V$是black-box，所以只能采取梯度逼近的方法。$G$和$S$一起组成了攻击模型。 关于Student model $S$的结构的选择 只需要知道目标模型所解决的问题的一般知识，选择适合的模型结构即可（根据知识蒸馏工作发现的结果） 关于损失函数的选择： 这里使用了$l_1$范数损失函数，$G$使用的损失函数与$S$的一致，但是要增大梯度（即增大$V$和$S$之间的差距） 关于目标模型$V$的梯度计算： $V$是个黑盒，所以采用梯度逼近的方法来计算其梯度。但是$S$的训练不依赖$V$的参数$θ_V$，但是$G$的训练需要。 具体的算法如下所示。在每个epoch中，$G$和$S$分别训练$n_G$和$n_S$次，$n_G$和$n_S$的大小关系是个tradeoff 3.2 Loss Function 选择的损失函数都是知识蒸馏问题中常用的 【注】$K$是类的数量 KL散度（相对熵）：即用概率分布 𝑞 来近似 𝑝 时所造成的信息损失量 但是当$S$越来越接近$V$时，KL散度就有梯度消失的问题了，导致训练$G$时，难以收敛 $l_1$范数损失函数（比KL散度简单），其中$v_i$和$s_i$都是softmax激活前的logits（即$w^Tx$）。但是$V$的参数不可知，所以需要logits（证明见附录） 【问题】这里计算$l_1$范数损失需要用到$V$的梯度吗 【问题】 3.3 Gradient Approximation（梯度逼近）用零阶优化的方法。优化$G$时，通过逼近图像$x$的梯度，再使用反向传播算法来优化$G$，这样逼近的梯度的维度就很小？？？？没明白 并使用前向差分法（forward differences method）来逼近梯度（即微分） 其中$u_i$是随机的方向向量，共$m$个 有限差分法也可以选用 【问题】零阶优化是一定要用到前向差分法吗，还是说梯度逼近需要用到前向差分 3.4 从概率分布中还原logits首先，计算 接着计算 4 Experimental Validation5 Ablation Studies 损失函数的选择 $l_1$范数损失函数要比KL散度损失函数好。KL散度损失函数收敛速度慢，而且会提前停止，存在梯度消失的问题 梯度逼近 在逼近梯度的过程中，方向向量$u_i$的问询次数$m$和梯度的准确度之间成正比关系，但是$m$越大，训练时候的开销就越大，所以这也是个tradeoff问题。在训练过程中，训练$S$使用的问询次数的占比为$\\frac{n_S}{n_S+(m+1)n_G}$ 作者提出了一种混合模式：先从一个surrogate dataset中提取出一个可能较差的student model，然后再用DFME中的算法，通过对目标模型的问询，来优化提取出的student model logits的准确率的影响 当logits的均方差MAE比较小的时候，就可以从分布中还原出来","link":"/2022/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04/"},{"title":"model extraction相关论文汇总（部分）","text":"收集了model extraction领域内，被引次数较高的一些论文 论文 发表期刊&amp;时间 被引 连接 阅读情况 Stealing machine learning models via prediction {APIs} USENIX 2016 1340 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer Stealing hyperparameters in machine learning IEEE 2018 405 https://ui.adsabs.harvard.edu/abs/2018arXiv180205351W/abstract Towards reverse-engineering black-box neural networks 279 https://arxiv.org/abs/1711.01768 Stealing functionality of black-box models CVPR 2019 268 https://openaccess.thecvf.com/content_CVPR_2019/html/Orekondy_Knockoff_Nets_Stealing_Functionality_of_Black-Box_Models_CVPR_2019_paper.html PRADA: protecting against DNN model stealing attacks IEEE 2019 255 https://arxiv.org/abs/1805.02628 High accuracy and high fidelity extraction of neural networks, USENIX 2020 169 https://www.usenix.org/conference/usenixsecurity20/presentation/jagielski Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures USENIX 2020 159 https://www.usenix.org/conference/usenixsecurity20/presentation/yan CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel USENIX 2019 142 https://www.usenix.org/conference/usenixsecurity19/presentation/batina","link":"/2022/10/26/research-model-extraction/"},{"title":"人脸识别门禁系统调研","text":"主要参考了四川大学一位博士的一篇毕业论文《高精度三维人脸识别技术及其门禁应用研究》中绪论及参考文献部分https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&amp;dbname=CDFDLAST2022&amp;filename=1021831289.nh&amp;uniplatform=NZKPT&amp;v=_CBMr2TIM4vECiK5CMJl0-VVf_M43nxrDEHrIj8VNBPFDF1AhJp9GCKcFnJK-09P 二维人脸识别【原理】对人脸的纹理数据进行特征表达，通过判断特征间的相似度来对人脸进行身份确认 【缺点】易受人脸姿态、妆容、环境光照等的影响，应用受限制 三维人脸识别门禁【优点】抗环境因素、人脸姿态因素、人脸化妆因素等 【原理】需要有数据采集、人脸识别、人脸防伪过程。 人脸识别技术通过数据采集前端抓取通行人员的图像信息，利用人脸检测算法获取人脸区域，并使用人脸识别算法模型对人脸区域图像进行特征表达，最后通过特征之间的相似度比较完成对通行人员的身份鉴定。人脸防伪使用人脸防伪模型对通行人员人脸数据是否为真人进行分类，进而完成对通行目标的是否为真人的判断。 基于深度学习的人脸识别通常的方法是对人像图像进行特征表达，在模型定义好的特征空间中根据特征间的距离计算出特征间的相似度，进而确定样本的人物身份。基于深度学习的人脸识别一般利用有监督的学习方法使用CNN对算法进行建模。使用迁移学习，使模型能够适应新的数据分布。 深度学习的人脸识别算法有：DeepID (香港中文大学)、DeepFace (Facebook)、FaceNet (Google)、VGGFace（牛津大学） DeepFace： 使用了三维模型来进行人脸对齐，然后使用深度卷积神经网络对人脸对齐后的人脸图像各个Patch进行分类学习，并使用经典的交叉熵损失函数对模型进行优化监督。 DeepID系列 DeepID：使用卷积神经网络对人脸进行表示，提取出其特征，使用softmax损失函数。其算法优化的主要手段就是增大数据集（对齐图片的预处理（增大数据集），再基于弱对齐图像从10个区域、3种尺度、RGB和灰色2种通道方面截取出60个面部块，分别输入单独的ConvNet，将从中提取特征。）检测的关键点是使用Sun等人提出的面部点检测方法检测5个面部标志点，包括两个眼睛中心，鼻尖和两个嘴角。 DeepID2：依然使用卷积神经网络对人脸进行表示，在损失函数上添加了验证信号（人脸识别信号和人脸验证信号），两个信号使用加权的方式进行了组合。。根据全局对齐的人脸和人脸标志点的位置，裁剪了400个patch，用200个ConvNet提取出400个特征。 DeepID2+ DeepID3：提出了2种深度神经网络框架（一个基于VGG，一个基于GoogLeNet，含有Inception层），监督信号作用于中间层和最后层。 FaceNet： 并没有2维和3维之间的对齐。使用GoogLeNet V1模型，冰使用三元组损失函数代替之前的交叉熵损尖函数，在一个超几何球空间上完成优化任务，使得类内距离更紧凑，类间距离更远。 这三种算法的详细介绍见后 VGGFace：人脸分类器使用VGGNet+Softmax loss，输出2622个类别概率，这是预训练过程。三元组学习人脸嵌入使用VGGNet+Triplet loss，输出1024维的人脸表示。在人脸分类器基础上，用目标数据集用fine-tuning方法学习了映射层。人脸验证则是通过比较两个人脸嵌入的欧式距离来验证同种人。 Center-Loss人脸识别 很好的特征收敛性能46 立体视觉法 人脸防伪除了要鉴定人脸对应的人物身份，还要人脸防伪，即鉴别是否是真实人脸（尤其要抵抗现在3D打印的、仿真度极高的三维面具） 在基于深度学习的人脸识别中，有CNN结合LSTM的方式使用多帧数据输入模拟LBP-TOP的方法。 三维人像的数据采集使用三维人脸照相机采集人脸数据，有直接的三维深度传感器，还可以使用三维重建算法对二维图像进行三维重建。三维人脸重建技术分为被动和主动2种 三维重建算法有基于纹理信息的、基于纹理阴影信息的、基于统计模型的、立体视觉法（研究最多、应用最广泛） 近年来，基于结构光的三维测量技术也已经广泛应用于商业产品中。 三维人脸采集的设备，比较著名的有美国Artec3D、3dMD、FARO等，国内有北京天远、讯恒图像、技睿新天、杭州先临等。 基于深度学习的人脸识别相关研究【Facebook】DeepFace: Closing the Gap to Human-Level Performance in Face Verification 发表于CVPR 2014，深度学习人脸识别的开山之作，主要用于人脸验证 https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf 参考了CSDN上的博客：https://blog.csdn.net/DL_wly/article/details/92850494 知乎文章https://zhuanlan.zhihu.com/p/76520981 人脸识别的一般流程为：检测——对齐——表示——分类 在人脸对齐过程中，DeepFace提出了新的方法： 用LBP（Local binary patterns, 局部二值模式）+SVR（Support Vector Regression，支持向量回归）的方法检测出人脸的6个基准点，眼镜两个点，鼻子一个点，嘴巴三个点，如下图(a) 通过拟合一个对基准点的转换（缩放，旋转，平移）对图像进行裁剪，得到下图(b) 对图像定位67个基准点，并进行三角剖分，得到下图(c) 用一个3D人脸库USF Human-ID得到一个平均3D人脸模型（正脸），如图(d) 学习一个3D人脸模型和原2D人脸之间的映射P，并可视化三角块，如图(e) 通过相关的映射，把原2D人脸中的基准点转换成3D模型产生的基准点，得到如图(f)所示，最后的正脸就是图(g)。 在人脸表示过程中，如下图所示，训练了一个DNN来提取人脸图像的特征表示 C1和C3表示卷积层，M2表示最大池化层，“32x11x11x3@142x142”表示使用32个大小为11x11x3的卷积核，输出feature map的大小为142x142。前三层主要提取低水平特征，其中最大池化可以使输出对微小的偏移更加鲁棒（可能人脸对齐歪了一些也没关系），因为最大池化会损失信息所有没有使用太多。 L4，L5，L6是局部卷积层，对于feature map上每个位置，学到不同的卷积核（即一张feature map上的卷积核参数不共享），因为人脸的不同区域会有不同的统计特征，比如眼睛和眉毛之间的区域比鼻子和嘴巴之间的区域具有更高的区分能力。局部卷积层会导致更大的参数量，需要很大的数据量才能支撑的起。 F7和F8是全连接层，用来捕捉（不同位置的）特征的相关性，比如眼睛的位置和形状，和嘴巴的位置和形状。F7层的输出提取出来作为人脸特征，和LBP特征对比。F8层的特征喂给softmax用于分类 对F7层的输出特征进行归一化（除以训练集上所有样本中的最大值），得到的特征向量值都为0到1之间 后面三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因： 对齐的人脸图片中，不同的区域会有不同的统计特征，卷积的局部稳定性假设并不存在，所以使用相同的卷积核会导致信息的丢失 不共享的卷积核并不增加抽取特征时的计算量，而会增加训练时的计算量 使用不共享的卷积核，需要训练的参数量大大增加，因而需要很大的数据量，然而这个条件本文刚好满足。 【Google】FaceNet https://arxiv.org/abs/1503.03832 参考知乎文章https://zhuanlan.zhihu.com/p/76538002 使用Triplet Loss（三元组损失函数）作为损失函数，基于GoogLeNet或Zeiler＆Fergus模型，直接学习从人脸图像到紧凑的欧几里德空间的映射，提取的嵌入特征。将提取到的特征进行L2 normalize，得到embedding结果（即一张图片使用128维向量表示）。再将得到的embedding结果作为输入，计算triplet loss。 Triplet Loss的定义如下。Triplet 三元组指的是：anchor, negative, positive 三个部分，每一部分都是一个 embedding 向量。其中anchor指的是基准图片，positive指的是与anchor同一分类下的一张图片，negative指的是与anchor不同分类的一张图片。训练目标：anchor与positive的距离比anchor与negative的距离小（相似度高）","link":"/2022/11/04/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E9%97%A8%E7%A6%81%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%A0%94/"},{"title":"人脸识别算法调研","text":"调研最新的人脸识别算法 基本概念 Detection is the ability to detect if there is ‘so mething’ vs nothing. 即判断存在性 Recognition is the ability to recognize what type of thing it is (person, animal, car, etc.). 识别类型。但是facial recognition一般还是指能够通过比较和分析基于人的面部轮廓的模式来独特地识别或验证一个人。 Identification is the ability to identify a specific individual from other people. 识别具体的个例（人） Authentication：Facial Authentication（面部认证），即验证一个人是否是他/她声称的那个人 人脸识别算法经典算法 【知乎网友的总结】https://www.zhihu.com/people/mengcius/posts?page=2 2014【Facebook】DeepFace: Closing the Gap to Human-Level Performance in Face Verification 2014【港中大】Deep Learning Face Representation from Predicting 10,000 Classes 2014【港中大】Deep Learning Face Representation by Joint Identification-Verification 2014 【港中大】Deeply learned face representations are sparse, selective, and robust 2015【Google】FaceNet: A Unified Embedding for Face Recognition and Clustering（Tripled Loss） 2015【港中大】DeepID3: Face Recognition with Very Deep Neural Networks 2015【牛津】VGG ==2017【佐治亚理工】SphereFace: Deep Hypersphere Embedding for Face Recognition== ==2018【腾讯】CosFace: Large Margin Cosine Loss for Deep Face Recognition== ==2018【伦敦帝国理工】ArcFace: Additive Angular Margin Loss for Deep Face Recognition== pdf、code “目前最为主流的，在学术界达到SOTA，在工业界也广泛使用”。 使用angular margin based losses： 其中，$\\theta_{j}$是$f_i$和第$j$个类的中心$w_j$之间的角度，$m$是 additive angular margin，$s$是缩放参数。 上大&amp;京东AI研究院综述 发表于2021年 https://arxiv.org/pdf/2009.13290.pdf 端到端人脸识别系统的流程，分为人脸检测(Face Detection)，面部对齐(Face Alignment)，人脸表示(Face Representation)，最后就是计算出相似度来判断是否是同一人。 Face Detection 有以下几类：multi-stage, single-stage, anchor-based, anchor-free, multi-task learning, CPU real-time和 problem-oriented methods 性能的测试如下： Face Representation感觉这些就是比较流行的人脸识别算法 最新研究 现在比较主流的人脸识别算法有ArcFace、Sphereface、CosFace(Additive Margin Softmax)用基于Margin的softmax损失函数，CurricularFace等是用基于mining的softmax。主流的模型有FaceNet、ImageNet AAAI 2020 Mis-classified vector guided softmax loss for face recognition pdf、code CVPR2020 ==CurricularFace: adaptive curriculum learning loss for deep face recognition== pdf、code 提出了Adaptive Curriculum Learning loss (CurricularFace)，在训练的不同阶段给不同困难程度的问题不同的重要性。 损失函数为： 其中， 算法如下所示： 这个算法对模型也没有提出要求，实验中主要使用ResNet100和ResNet50 ICCV 2021 SynFace: Face Recognition with Synthetic Data pdf、code 提出了用合成的人脸图像来进行模型训练的方法。 模型首先使用DiscoFaceGAN组成混合人脸生成器，可以混合两张生成图像。之后，生成的图象与一部分真实图象混合。然后，特征提取器以混合人脸图像作为输入，提取相应的特征。提取的特征要么用于计算模型训练的基于边际的softmax损失(其中W1、W2是两个不同类别的中心权重向量，x是特征向量)，要么作为人脸表示来执行人脸识别和验证任务。 NeurIPS 2021 Fair SA: Sensitivity Analysis for Fairness in Face Recognition pdf、 以扩展 VPSA 的通用框架的形式提出了一种基于鲁棒性的新公平性评估 CVPR 2021 When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework pdf、code 首先，提出了一个统一的多任务学习框架来联合处理AIFR和FAS，该框架可以学习年龄不变的身份相关表征，同时实现效果好的人脸合成。其次，提出了一种基于注意力的特征分解方法在高级特征maps上分离与年龄相关和身份相关的特征，与之前对特征向量的无约束分解相比，该方法可以约束分解过程。结合年龄估计和人脸识别任务，同时结合连续域自适应来监督分解过程。第三，与之前实现年龄group-level人脸转换的one-hot编码相比，提出了一种新的身份条件模块来实现identity-level人脸转换，并通过权重共享策略提高合成人脸的年龄平滑度。 ==MagFace: A Universal Representation for Face Recognition and Quality Assessment==pdf、code 在ArcFace的基础上，提出了MagFace（一种改进的损失函数），它在类间间隔之外，充分利用方向(direction)和模长(magnitude)两个维度的信息来衡量人脸质量问题，有更好的类内分布，也有利于聚类问题。这可以防止模型在嘈杂的低质量样本上过度拟合，并改善野外的人脸识别。是对识别loss进行优化，不需要额外的标注或者网络结构调整，即插即用。 MagFace对ArcFace的改进就是将additive angular margin $m$替换成了基于特征模长的函数$m(a_i)$，并且加入了一个基于特征模长的惩罚项$\\lambda_gg(a_i)$，这是分类损失和归一化损失之间的tradeoff。 WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition pdf 一个新的百万级人脸基准 ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis pdf 目前公开最大的深度人脸伪造数据集 ==Spherical Confidence Learning for Face Recognition== pdf、code 19年ICCV，PFE首先将不确定性引入了人脸识别中，假设特征为多元独立高斯分布。但是由于人脸特征归一化破坏了多元独立高斯分布的假设，导致训练不稳定。SCF的改进主要就是更换了分布，从多元独立高斯分布变为超球面上的vMF分布。提出了在spherical space上进行人脸确定性学习的框架：将von Mises Fisher密度扩展到r-半径，并推导了优化目标的闭式解，可解释性更强。 CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement pdf 【Face Detection】propose a confidence ranking network by taking the bounding box and confidence prediction from a face detector to output new refined confidences. Cross-Domain Similarity Learning for Face Recognition in Unseen Domains pdf 【跨域小样本学习】引入了一种新的跨域度量学习损失：Cross-Domain Triplet (CDT)损失，以提高未知区域的人脸识别 Dynamic Class Queue for Large Scale Face Recognition In the Wild pdf、code （注：CSIG FAT-AI 2021 masked face recognition challenge冠军） 作者提出 dynamic class queue（DCQ）来解决计算资源成本和长尾分类问题。 Consistent Instance False Positive Improves Fairness in Face Recognition pdf、code 基于softmax损失，提出了一种新的损失函数：假阳性率惩罚损失，该损失函数通过提高实例 False Positive Rate（FPR）的一致性来减轻人脸识别的偏差。该方法不需要人口统计学标注，可以减轻由各种属性划分的人口群体之间的偏差，而且这些属性不需要在训练中预定义。 Mitigating Face Recognition Bias via Group Adaptive Classifier pdf、code 本文设计一个特殊的卷积操作，允许这个卷积操作可以根据输入样本的人种属性进行自适应调整，最终做到为不同的人种设计不同的特征提取计算。把人脸上能提取出来的特征划分成两类：所有人脸巩共有的通用特征，和与人种相关的特有特征。使用各个人种对应的独有模式来提取特征，并且使用通用模式来提升模型泛化性。基于这个思想提出模型group adaptive classifier（GAC）。它包含适应层adaptive layer和自动化模块automation module。适应层中包含适应卷积核和通道注意力两部分。 首先，使用一个人口统计学属性分类器Demographic Attribute Classifier来获得输入图片的人种标签。然后就是2个模块适应模块adaptation module和自动化模块automation module（先不继续深入研读了） Variational prototype learning for deep face recognition pdf、code 建立memory bank存储历史人脸特征，在计算softmax-based loss时，将历史人脸特征与分类权重加权融合，得到动态可变的分类权重(Variational Prototype)，等于用一个分类层完成了pair-based比对和class-based优化。 CVPR 2022 DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover’s Distance Improves Out-Of-Distribution Face Identification pdf、code Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin pdf、code 【表情识别】 An Efficient Training Approach for Very Large Scale Face Recognition pdf、code 本文提出的方法在一定层度上缓解大数据量情况下，dataloader和FC层（全连接层）参数量的限制。本文提出的方法为 Faster Face Classification($F^2C$)，运用Dynamic Class Pool（动态类池，DCP）来动态地保存和更新特征，可以将DCP视为FC层的替代。 ==AdaFace: Quality Adaptive Margin for Face Recognition== pdf、code 提出了一种新的损失函数来通过图像质量强调不同的困难样本的重要性。本文的方法通过用feature norms来近似图像质量，这里是以自适应边缘函数的形式来实现这一点。 本文提出的损失函数和基于Margin的Softmax损失函数之间的区别如下所示。 图像质量的指标： ，是将图像的feature norm进行了正则化，其中$\\mu_z$和$\\sigma_z$分别是一个batch中$||z_i||$的平均值和标准差。$\\left \\lfloor · \\right \\rceil_{-1}^1$在-1和1之间截断数值，并停止梯度的流动。 Adaptive Margin Function： 这个方法对模型的结构其实没有要求，实验中使用的是ResNet50、ResNet100等模型 p.s.文中这张表还挺好的，基本上总结了现在比较流行的人脸识别算法的损失函数 Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC pdf、code 提出了PFC(一种FC的变体)，来解决类间冲突和长尾分布问题。该方法在每次迭代中，都会选择正类中心和一个随机的负类中心子集来计算基于margin的softmax损失。在整个训练过程中，所有的类中心仍然保持不变，但在每次迭代中只选择和更新一个子集。因此，计算要求、类间冲突的概率以及对尾部类中心的被动更新频率都大大降低。 Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing pdf、code 【人脸防伪】 提出了Shuffled Style Assembly Network (SSAN) ，提取并重新组合不同的内容和风格特征，形成一个风格化的特征空间。content information主要记录一些全局语义特征和物理属性。style information保留了一些有利于加强区分活体和欺骗的鉴别性信息。总体结构如下所示： 内容特征的提取： 使用梯度反转层(GRL)来优化内容特征生成器和领域判别器(content feature generator and domain discriminator) 风格特征的提取： 接着，使用style assembly layers (SAL)来将内容特征和风格特征结合起来。SAL的建立使用AdaIN layers和卷积运算： 其中， ECCV 2020 Explainable Face Recognition pdf、code 该论文的贡献可以归结为如下三点，分别如下所示 XFR baseline：作者基于五种网络注意力算法为XFR（人脸识别的可解释性）提供了baseline，并在三个用于人脸识别的公开深度卷积网络上进行了评估：LightCNN、VGGPFACE2和SNET-101。 图像修复游戏协议和数据集：作者提供标准化评估协议和数据集，用于细粒度的人脸识别可视化。这为客观地比较XFR系统提供了一个量化指标。 XFR评估：作者首次对图像修复协议的baseline算法进行了全面的评估，从而得出关于这些方法在真实图像上解释的实用性的结论。","link":"/2022/11/21/2022-11-24%20%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E8%B0%83%E7%A0%94/"}],"tags":[{"name":"Model Extraction","slug":"Model-Extraction","link":"/tags/Model-Extraction/"},{"name":"Model Inversion Attack","slug":"Model-Inversion-Attack","link":"/tags/Model-Inversion-Attack/"},{"name":"Membership Inference Attack","slug":"Membership-Inference-Attack","link":"/tags/Membership-Inference-Attack/"},{"name":"数据重建","slug":"数据重建","link":"/tags/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%BB%BA/"},{"name":"model extraction","slug":"model-extraction","link":"/tags/model-extraction/"}],"categories":[],"pages":[]}