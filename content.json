{"posts":[{"title":"【论文笔记】Plug &amp; Play Attacks—— Towards Robust and Flexible Model Inversion Attacks（ICMP 2022）","text":"https://zhuanlan.zhihu.com/p/536091331 摘要关于Model Inversion Attacks（模型反转攻击），即通过利用模型的learned knowledge，从目标分类器的私有训练数据中创造出反映class特征的合成图像。相当于从模型中提取出每种类别特定的特征。这也是很严重安全隐患，因为可能可以提取出如人脸、指纹、身份信息等敏感隐私信息，那么攻击者可以重构人脸，冒用身份…… 以前的研究都是训练GAN（生成对抗网络）来作为image priors（先验）。但是存在耗时、耗力、易受数据集分布变化的影响。本文提出的Plug &amp; Play攻击可以减少对image prior的依赖，只需要一个GAN网络即可对大范围的目标进行攻击。而且即使使用的是预训练好的GAN模型、数据分布发生很大变化也可以达到很好的攻击效果。 1 Introduction为了解决训练过程中会出现的梯度消失的问题，使用Poincare损失函数，而不是交叉熵损失函数。加入随机转化避免过拟合。首次提出了要从攻击结果中找出有意义的样本的重要性。 2 Model Inversion in Deep Learning 介绍了以前一些攻击方法，及其缺陷。 目前有三种攻击方式：optimization-based, training-based, or architecture-based optimization-based：通过产生合成的模型输入来找类型特征，也基于梯度下降的方法 training-based：训练GAN模型，将目标模型视作一个加密器，要找解密器 这些都不是为了获取隐私信息，只是探寻目标模型各个类别的特征 可能存在fooling image（无意义的噪点图像），out-of-distribution data（如猫狗分类中出现了一个熊猫图像） 影响因素：distributional shifts, vanishing gradients, and non-robust target models 3 Generative Model Inversion Attacks 定义了理想的MIA攻击，以及会影响MIA攻击效果的因素 假设：目标分类模型为，且为对x是否为类c∈C的预测分数，敌手可以访问目标模型（作为white box），且无限次地问询，但并不知道C的任何信息。 敌手要构造一个合成图像x^，满足时目标类别c的特征，泄露了隐私信息。这在较浅的神经网络中可行，但是现在流行的深层神经网络就不行了。 之后产生了使用GAN网络来生成样本（将特征向量z映射到图像空间），并训练鉴别器D来鉴别是否是真实样本x还是由G生成的G(z)，得到等式。这样或许可以得到评分（$y_c=M_{target}(\\overset{\\thicksim}{x} )_c$）最高的$ \\overset{\\thicksim}{x} = G(\\overset{\\thicksim}{z})$。但是这个构造得到的$\\overset{\\thicksim}{x}$不一定具有意义。 为图像的分布，令为X中的样本可能具有的人类可识别特征的分布，令为c类的特征。如再面部图像中，F和P的区别在于，F可能包含发色、皱纹、瞳孔间距等面部特征，而P还多了与身份信息无关的（如服装、背景等）。也就是说，$F$只包含和人物的身份信息有关的信息，而$P$还包含了无用的信息。 注意，不同类别之间的特征可能相同。 现假设，有2个类别满足，且，即特征不相同。 G可以拟合P(X)，进而近似F(X)，然后合成符合这个分布的样本x。 【问题】$\\overset{\\thicksim}{X}$是如何得到的 4 Towards Robust and Flexible MIAs 介绍本文提出的Plug &amp; Play攻击方式。这种方式鲁棒性更好，也更适用于distributional shift setting分布转换设置 首先，latent vectors潜伏向量被采样并映射到它们的中间表征w（用预训练的StyleGAN2模型） 然后根据w生成图像，进行转换（Transformations），并输入目标模型。 最后，在目标模型的输出和目标类别c上计算Poincare损失，并通过反向传播损失和执行梯度下降步骤更新w。优化完成后，根据其对随机变换的鲁棒性，选择一个结果子集。 4.1. Target-Independent Image Priors采用预训练的StyleGAN2模型作为图像的先验（不需要辅助输入和训练特定数据集的模型）。只需要是同一领域预训练好的模型。 这里预训练好的StyleGAN2包含2个模块：会将随机的latent vector潜伏向量（服从标准正态分布）的z映射到intermediate latent representation中间潜伏向量w。则根据w来生成图片 4.2. Increasing Robustness by Transformations进行一系列的图像变换，并令。在优化过程中，先进行这些可导的图像变换，再输入到目标模型中得到预测分数。即在前向传播过程中计算的是 这样，若这些变换后的图像越接近目标分布，攻击就会越成功。而且可以增加生成的图像的鲁棒性 4.3 Overcoming Vanishing Gradients以前的MIA都基于交叉熵损失函数，这样关于output logit的偏导为，容易发生梯度消失。而初始的潜伏向量又是随机产生的，随着调整，其预测分数会越来越高，导致梯度趋于消失。这样如果预先的采样不好的话，由于梯度下降的问题，对初始图像的改动就很小，攻击就会不成功。 为了解决这个问题，采用Poincaré distance作为损失函数。（其中$||·||_2$是欧几里得范数）。它是一个特殊hyperbolic space(双曲空间)中的距离度量。在这个空间里面，一个圆的面积随着半径的增加指数型增加。 【注】欧几里得范数即$L2$范数 4.4 Selecting Meaningful Attack Results 选用的方法是transformation-based selection，即通过变换，重新计算得分，选择最高的 攻击结果可能存在误导。本文采取的方法如下： 选取大量的符合正态分布的样本，并映射到intermediate latent space中间隐藏空间$W$ 为每一个w生成图像，并进行剪裁和调整大小的变换。计算平均预测分数，以及horizontally flipped counterpart with $M_{target}$，为每个类别c选择其中得分最高的初始图像。 根据蒙特卡罗方法 计算预期稳健预测分数，并进行$N=100$次随机的图像变换，选取其中得分最高的50个作为最终攻击结果。【注意】这里选取的变换要和优化过程中选取的不同或更强（否则poorly generated的样本可能会过拟合） 总结Plug &amp; Play攻击要解决的问题就是： 5 Experiments 介绍了验证Plug &amp; Play有效性的实验，和其他攻击方法的对比 评价标准有： 在目标模型的数据集上训练Inception-v3模型，然后用该模型对攻击结果进行打分，给出在目标类别上的top-1和top-5准确率。 计算平均特征距离$δ_{eval}$，针对面部图像用预训练的FaceNet来测量特征距离$δ_{face}$，越小说明越接近训练数据。 Fréchet inception distance（FID）","link":"/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment aaaaa","link":"/2022/10/18/hello-world/"},{"title":"【论文笔记】Membership Inference Attacks Against Machine Learning Models (ICMP 2017)","text":"研究成员推理攻击（Membership Inference Attack, MIA）：目标模型作为black box，要求判断某一条data record是否是用于训练该模型的数据。并探讨导致泄露的因素 本文主要针对有监督训练的模型，提出了shadow training的技术来为训练攻击模型构造数据集：首先，利用训练目标模型的API训练若干shadow models，模仿目标模型的行为接着，用训练好的shadow models来为attack model构造训练集数据、测试集数据（解决了目标模型是black box的问题，对预测向量的数据增强效果）最终，attack model实现判断是否是训练集的二分类问题 攻击的实现主要是利用了目标模型在其训练集和非训练集上的输出行为差异，这一般由于训练时发生了过拟合导致，但其根本原因在于存在accuracy gap（模型在训练集和测试集上准确率的差异，对每个类别来说） 1 Introduction目前商用的机器学习模型都是以a black-box API的形式来提供服务（例如根据用户喜好训练的推荐模型等等）。本实验通过训练一个攻击模型来实现成员推理攻击，通过区分目标模型对于训练数据和非训练数据的不同输出行为。 当模型是一个black box时，模型的结构、参数就是不可知的了（是white box时就是都知道的，之前2篇论文中都是以white box的形式来访问） 为了更好地实现攻击，引入了shadow training的技术：首先，创建多个模仿目标模型行为的“shadow models”，但这些模型的训练数据集是已知的，因此知道这些数据集中成员关系的基本真相。再用这些标记的输入和shadow models的输出来训练攻击模型 注意和model inversion的区别：model inversion是为了重建训练数据，或者从模型中得到某些class的属性、特征。而membership inference是为了判断某个数据是否是用于训练该模型的数据。也就是说，model inversion不关心单个数据record 2 Machine Learning Background商用的训练模型由厂商选择，可能只基于自己的验证子集来设置模型的结构，那么在用户自己的数据集上训练时，很可能产生过拟合。即使现在有很多使用正则化来解决过拟合问题的方法，但是在商用模型中很少会控制正则化。 过拟合就会导致训练数据和非训练数据之间有明显差异 3 Privacy in Machine Learning 定义了研究的对象membership inference，强调了和model inversion之间的区别 抵御membership inference attack是为了保护个体的隐私安全。而抵御model inversion是要保护一整个类的隐私信息（本文中认为model inversion很难抵御，例如某个疾病和某个基因有关，那么判断出有这种疾病就可以推断患者的这个基因，即class的特征是必然会被模型的判断结果所泄露的） 本文研究的membership inference attack是要评估一个人的membership risk，即若他的个人数据被用来训练一个模型（如推荐系统），是否会泄露其个人信息 4 Problem Statement 描述了研究的问题，前提假设条件 针对分类模型 假设敌手可以获得输出的prediction vector（在每一个类别上的打分）；知道输入输出的形式、取值范围、知道模型的结构、算法或者可以访问训练模型的learning oracle（二选一，后者不知道具体的参数、结构）；知道训练集分布的背景知识（如有相同的分布的不相交子集、或者知道其边际分布 批判标准：precision（精确率，判断为真的中有多少是对的）和recall（召回率，多少真的被判断为真的了） 5 Membership Inference 具体的攻击方法 A. Overview of the attack B. Shadow models攻击者要创建$k$个shadow model $f^{i}_{shadow}()$，训练的数据集为$D^{train}_{shadow^i}$（和目标模型的数据集的分布相似，但是不相交。虽然在商业模式下的目标模型的训练算法和结构不可知，但是可以同样利用这个商业服务来训练shadow model（如下图所示，其中$train()$就是服务商提高的API） shadow model越多，攻击模型的准确度就越高。那么接下去的问题就是用于训练shadow model的数据$D^{train}_{shadow^i}$如何而来 C. Generating training data for shadow models 介绍了几种生成目标模型训练集分布接近的数据集的方法 Model-based synthesis（使用目标模型来构造数据集） 分为2个阶段 （1）用hill-climbing算法$search$可能数据的空间，即随机生成$x$，选择其中得分高的（针对具体的某个类c） （2）从上述数据集中$sample$合成的数据 在$search$（迭代过程）时，设置了$j$来记录失败次数，$k$用来控制围绕被接受的记录的搜索直径，以便提出新的记录（每次提出新的x时，只替换k个特征，即搜索半径） 当$x$的评分向量$y$中$y_c$最大（表明这个$x$是$c$类的），而且$y_c$超过了预先设定的指标$conf_{min}$，那么就选择这个$x$ Statistic-based synthesis 若敌手知道训练集的统计信息，那么直接从这个分布中合成数据即可 Noisy real data 若敌手可访问和训练集类似的数据集，然后随机反转其中10%或20%的特征，就可以作为训练shadow models的数据集了 D. Training the attack model主要的思想就是在相似的训练集上以相同方式训练的模型，其行为是相似的 给shadow models一系列输入，得到其相应的输出，若是shadow model对应的训练集中的数据，就标记为$in$，若不是，则标记为$out$。这些记录用于训练attack model，并分成$c_{target}$个划分，分别对应目标模型的$c_{target}$个类别。 问询shadow models时用的测试集和训练集是不相交的（否则有歧义）这里训练的attack model要解决的是二分类问题，所以任何适用于二分类问题的模型都可以（一般常用的模型，或者服务商提供的API都可以）（感觉思想挺简单的，但是要实现比较复杂，有好多shadow models，而且训练shadow models时的数据集很重要） 6 Evaluation 介绍实验中使用的数据集、目标模型，展示实验结果 实验结果表明：本文所提出的攻击方法的鲁棒性很好，即使对目标模型的训练集的分布猜测有误，攻击的准确性还是很高。 【!】当类的训练数据在整个训练集的比重较小时，攻击在这些类上的精确度（precision）上较低。 7 类的数量，和每个类的训练数据集对攻击的影响类的数量越多，泄露的模型的内部状态越多。因为当数据集中类越多，目标模型就需要提取出更多特征、“记住”更多数据的信息来准确判断类别，所以也就会泄露更多信息。 【？？？】为什么这里说 “the more data in the training dataset is associated with a given class, the lower the attack precision for that class” 但是之前上面说“The reason for the attack’s low precision on some classes is that the target classifier cannot confidently model the distribution of data records belonging to these classes—because it has not seen enough examples.” 感觉有点矛盾？ 过拟合对攻击的影响同一种类的模型中，过拟合程度越大，模型泄露的信息越多。但是相同过拟合程度下，不同模型的信息泄露程度不一样。【!】所以，过拟合不是导致易受Membership Inference Attack的唯一原因。 模型结构中很重要的因素是accuracy gap（模型在训练集和测试集上准确率的差异，对每个类别来说）。当一个类别在某个模型中的accuracy gap越大，攻击在这类上的精确度（precision）越高。 毕竟MIA就是要区分目标模型在测试集和训练集上的行为差异，那么当accuracy gap越大，就说明二者的差异越大，攻击自然就越容易成功了，。 8 MIA成功的原因与抵御MIA的成功与否与目标模型的普适性（generalizability）&amp;训练集数据的多样性有关。 因为过拟合会显著导致易受MIA攻击，所以解决过拟合问题的方法是可以用来抵御MIA的。常见的方法由dropout（）、Regularization（正则化）。 从训练过程角度，使用差异化的私有模型（differentially private models，就是和差分隐私有关的），这样一个模型是包含一条数据训练而得的概率和包含这条数据而得的概率相近，这样也可以用来抵御MIA。 从这些商用模型训练API的服务商角度，应该告知用户过拟合的风险、为数据集提供更加适合的模型结构等等。 A. Mitigation strategies几种抵御MIA的策略如下 Restrict the prediction vector to top k classes Coarsen precision of the prediction vector Increase entropy of the prediction vector Use regularization 根据作者的实验结果，只有正则化比较有用。可见在训练模型时，正则化非常重要！！","link":"/2022/10/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/"},{"title":"【论文笔记】Reconstructing Training Data with Informed Adversaries（S&amp;P 2022）","text":"摘要&amp;介绍由于机器学习模型可以有“记忆”功能，那么当训练数据涉及隐私信息时，若被攻击者还原出原始训练数据，就会很危险。这个论文就是探讨这样的攻击是否可能。 所有的训练数据中，只有一个是未知的。目标就是还原出这一个未知的数据样本，找到攻击的方式、泄露多少信息可以达到被还原、哪些特性会导致可还原、自我检查模型的安全性（不泄露信息） 本文提出了一种研究ML模型重构攻击的可行性的通用方法，而无需假设模型的类型或访问中间梯度，并启动了一项能够防止这类攻击的缓解策略的研究 RecoNN一种用于重现数据而训练的神经网络 II. Reconstruction with Informed Adversaries该部分主要介绍了为什么定义了这样很强的infromed adversaries，即敌手知道除了target以外的所有数据。以及这样的reconstruction攻击和membership inference攻击、attribute inference攻击之间的关系。 III. Reconstruction in convex settingA. Reconstruction Strategy for Convex Models针对ERM（经验风险最小化问题）下凸监督学习模型的攻击，在没有side knowledge的时候，对线性回归、岭回归、Logistic回归都可以实现攻击。 若风险函数为，且，则最优解为 敌手有访问模型的white box（说明知道模型的结构、参数信息），并且有其他所有样本D_ 的信息，那么target z就满足一个方程式 监督学习中一个样本用(x,y)即d维的特征向量x和1维的标签y来表示，则要从d’个等式中解出d+1个未知数（p.s. d’是模型空间的维度） 【解读】为什么是d’个等式？因为模型空间中，每个模型都有一个方程 B. Closed-Form Reconstruction Against GLMs 证明在用截距项拟合的GLM（广义线性模型）的情况下，存在这种攻击的闭式解决方案。 这个攻击下，informed敌手不需要其他关于z的side knowledge 【证明】 IV. A General Reconstruction Attack 针对一般的机器学习(ML)模型的重建攻击 直观上，$z$对模型$θ=A(D_∪{z})$的影响和$\\overset{\\thicksim}{z}$对$\\overset{\\thicksim}{θ}=A(D_ ∪\\overset{\\thicksim}{z})$的影响是一样的，所以可以通过反复尝试来找到$z$ A. General Attack Strategy记$A_{D_}=A(D_ ∪{z})$，那么当$D_$ 给定时，A就是Z→Θ的函数。那么reconstruction攻击就是要解出$z=A_{D_}^{-1}(θ)$ 【注意】这里是一般的ML模型，所以不一定能够保证是convex的，而且模型训练过程中可能有随机性 因为假设了敌手特别强大，所以可以采用枚举的方式，找到最接近θ的论文中采用的机制是using “neural networks to attack neural networks”，（用魔法打败魔法hhhh），构建了RecoNN模型 B. Training Reconstructor Networks本文中重构z的步骤如下 $\\bar D=\\{\\bar z_1,\\bar z_2,…,\\bar z_k\\}$是$Z$中的shadow targets，这属于关于$z$的side knowledge 枚举$\\bar D$中的所有元素，得到$\\bar θ_i$，从而得到了attack training data攻击训练数据 使用训练算法R得到RecoNN模型$Φ$ 应用$Φ$，得到结果$\\bar z$ 此外，假设了$X$和$Y$都是有限的，且$y$可以从$x$中直接推断出，那么只要重构样本的特征向量$x$即可 具体的训练方式见后面的讨论 using “neural networks to attack neural networks”的相关工作被应用于了membership inference，model inversion模型反演，property inference V. Experimental SetupA. Default Settings实验中使用的模型的超参数 a. 数据集的分割：$D_$、D^和测试目标集是三个不相交的集合 b. 生成模型的训练：训练方式采用的就是标准的梯度下降法，且使用了full batches。假设了敌手直到模型的初始状态，所有模型的初始状态都一致（之后也有讨论使用mini-batching和随机初始化情况下的攻击） MINST数据集上训练出来的模型准确率更加高（因为CIFAR-10更加复杂），所以对CIFAR-10的攻击可能更加困难，需要更加庞大的shadow points去训练RecoNN c. 重建模型的训练：When training the reconstructor, shadow model parameters across layers are flattened and concatenated together.？？？什么意思. 在训练的时候，跨层的参数被扁平化，且连接在一起。还需要把所有的坐标点都缩放到0均值和平均方差，因为有些参数很小。 针对MINST，采用MAE（平均绝对误差）+MSE（均方误差）的方式来计算shadow targets和reconstructor outputs之间的loss。 针对CIFAR-10，增加了LPIPS loss（感知损失）和GAN-like Discriminator loss，都是为了提高图像的视觉质量 B. Criteria for Attack Success介绍了实验中设置的评估指标 a. MSE：均方差 b. LPIPS：感知损失，即比较深度特征，更加接近人类视觉 c. KL散度： d. Nearest Neighbor Oracle: 一个oracle，可以猜测离z最接近的点z^ VI. Empirical Studies in reconstruction总结采用的策略是针对已知的所有训练数据和shadow targets训练得到的所有模型θ~i~来训练RecoNN模型Φ，来得到θ对应的样本z，即using “neural networks to attack neural networks” 相关知识 membership inference attacks (MIA)：成员推理攻击，即判断一个对象是否是模型训练时使用的数据https://blog.csdn.net/xff1994/article/details/89964552 attribute inference attacks (AIA) ：推理攻击 property inference attacks (PIA)：属性推理攻击，AIA的一种泛化攻击，一般都是得到样本数据的一些属性、平均信息，不会损害个人隐私 GLM广义线性模型：https://zhuanlan.zhihu.com/p/22876460 https://xg1990.com/blog/archives/304 例如线性回归中假设y|x服从正态分布，Logistic回归中假设y|x服从二项分布 ![](论文笔记1/image-20221008144438312.png) canonical exponential family 指数分布族 而在GLM中，假设y|x满足这种指数分布族是因为这样下的y|x的期望、方差都非常简单，而要求的目标函数h(x;θ)就是y|x的期望，这样就很简单了。 canonical link function 正则关联函数，是canonical response function（正则响应函数）的倒数 LPIPS loss（感知损失）：用深度特征来度量图像的相似度 https://zhuanlan.zhihu.com/p/206470186 GAN-like Discriminator loss GAN网络（生成对抗网络是要通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布） 问题 没明白”When training the reconstructor, shadow model parameters across layers are flattened and concatenated together.“ 扁平化就是说降低维度吧。","link":"/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01/"},{"title":"【论文笔记】Data-Free Model Extraction（CVPR 2021）","text":"（还不是很完善orz） 本文证明并解决了现有model extraction对数据集的依赖（依赖于替代数据集与实际训练集的相似性），并且在目标模型以black box形式被访问的条件下，提出了data-free的model extraction攻击，称为data-free model extraction (DFME)。DFME中采用了data-free knowledge transfer中的一些技术。 利用GAN网络来训练攻击模型，其中一个generator负责生成训练数据，另一个student model来学习目标模型在这些数据上的行为（做到data free） 使用$l_1$范式损失函数（防止梯度消失） 使用零阶优化（前向差分法）来逼近梯度（解决目标模型是个black box，无法获知其真实梯度的问题） 从概率分布中还原目标模型的logits 1 相关工作1.1 Data-Free Knowledge Distillation（无数据知识蒸馏）可参考https://zhuanlan.zhihu.com/p/516930757、https://zhuanlan.zhihu.com/p/102038521（知识蒸馏） 知识蒸馏是一种模型压缩方法，是一种基于“teacher-student网络思想”的训练方法，目标是把一个训练好的模型（teacher，Net-T）的知识压缩给另外一个模型（student，Net-S），达到节省模型训练的开销的作用。其中，Net-T一般结构复杂、参数量大，而Net-S结构较为简单、参数量小。那么在用Net-T来训练Net-S的时候，就是让Net-S去学习Net-T的泛化能力，直接的办法是使用softmax层输出的类别的概率来作为“soft target”。 soft target相对应的是hard target。一般的标签都是hard target，就是说一个对象只有一个标签。而soft target则是在每个标签上都有一定的值。举例来说，一个写的像3的2，在”3”上具有一定的概率值。而写的像7的2，在”7”上有一定的概率值，那么他们就提供了更多的信息。 【参考】https://antkillerfarm.github.io/dl%20acceleration/2019/07/27/DL_acceleration_6.html 使用soft target来训练Net-S时，softmax函数就变成了如下的形式 （具体先不展开，看看之后需要用到哪些KD相关的知识再说） 1.2 Generative Models本文中使用GAN类似的方法去训练student，用GAN去拟合最能说明当前student和teacher的决策曲面之间差异的数据分布（好复杂。。。） 1.3 Black-box Gradient Approximation因为目标模型是个black box，无法获得其梯度，所以采用的是零阶优化的方式来做到梯度逼近（之后会再介绍） 2 How Hard is it to Find a Surrogate Dataset?实验证明了现有model extraction都是基于替代数据集（surrogate dataset）非常接近与原始训练集的，若是替代集和原始训练集差别比较大，攻击的准确率就会低。 但是这样的surrogate dataset在现实中难以获得，于是作者提出了DFME 3 Data-Free Model Extraction 介绍了本文提出的攻击方法 DFME的目标是使extraction model对x的预测和目标模型对x的预测不同的概率最小。其中$D_V$是目标模型的训练集（攻击者不可知），所以可以用合成的数据集$D_S$ 训练extraction model时，优化时使用的损失函数是$L$，则要满足 3.1 Overview GFME的过程就是一个GAN网络，generator $G$生成输入$x$，目标网络$V$和Student网络$S$作为discriminator，$S$要去学习$V$对输入$x$的预测行为。其中$z$是一个随机的噪音，$G$通过给到的$z$来生成输入$x$。然后通过反向传播来优化。但是目标模型$V$是black-box，所以只能采取梯度逼近的方法。$G$和$S$一起组成了攻击模型。 关于Student model $S$的结构的选择 只需要知道目标模型所解决的问题的一般知识，选择适合的模型结构即可（根据知识蒸馏工作发现的结果） 关于损失函数的选择： 这里使用了$l_1$范数损失函数，$G$使用的损失函数与$S$的一致，但是要增大梯度（即增大$V$和$S$之间的差距） 关于目标模型$V$的梯度计算： $V$是个黑盒，所以采用梯度逼近的方法来计算其梯度。但是$S$的训练不依赖$V$的参数$θ_V$，但是$G$的训练需要。 具体的算法如下所示。在每个epoch中，$G$和$S$分别训练$n_G$和$n_S$次，$n_G$和$n_S$的大小关系是个tradeoff 3.2 Loss Function 选择的损失函数都是知识蒸馏问题中常用的 【注】$K$是类的数量 KL散度（相对熵）：即用概率分布 𝑞 来近似 𝑝 时所造成的信息损失量 但是当$S$越来越接近$V$时，KL散度就有梯度消失的问题了，导致训练$G$时，难以收敛 $l_1$范数损失函数（比KL散度简单），其中$v_i$和$s_i$都是softmax激活前的logits（即$w^Tx$）。但是$V$的参数不可知，所以需要logits（证明见附录） 【问题】这里计算$l_1$范数损失需要用到$V$的梯度吗 【问题】 3.3 Gradient Approximation（梯度逼近）用零阶优化的方法。优化$G$时，通过逼近图像$x$的梯度，再使用反向传播算法来优化$G$，这样逼近的梯度的维度就很小？？？？没明白 并使用前向差分法（forward differences method）来逼近梯度（即微分） 其中$u_i$是随机的方向向量，共$m$个 有限差分法也可以选用 【问题】零阶优化是一定要用到前向差分法吗，还是说梯度逼近需要用到前向差分 3.4 从概率分布中还原logits首先，计算 接着计算 4 Experimental Validation5 Ablation Studies 损失函数的选择 $l_1$范数损失函数要比KL散度损失函数好。KL散度损失函数收敛速度慢，而且会提前停止，存在梯度消失的问题 梯度逼近 在逼近梯度的过程中，方向向量$u_i$的问询次数$m$和梯度的准确度之间成正比关系，但是$m$越大，训练时候的开销就越大，所以这也是个tradeoff问题。在训练过程中，训练$S$使用的问询次数的占比为$\\frac{n_S}{n_S+(m+1)n_G}$ 作者提出了一种混合模式：先从一个surrogate dataset中提取出一个可能较差的student model，然后再用DFME中的算法，通过对目标模型的问询，来优化提取出的student model logits的准确率的影响 当logits的均方差MAE比较小的时候，就可以从分布中还原出来","link":"/2022/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04/"},{"title":"model extraction相关论文汇总（部分）","text":"收集了model extraction领域内，被引次数较高的一些论文 论文 发表期刊&amp;时间 被引 连接 阅读情况 Stealing machine learning models via prediction {APIs} USENIX 2016 1340 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer Stealing hyperparameters in machine learning IEEE 2018 405 https://ui.adsabs.harvard.edu/abs/2018arXiv180205351W/abstract Towards reverse-engineering black-box neural networks 279 https://arxiv.org/abs/1711.01768 Stealing functionality of black-box models CVPR 2019 268 https://openaccess.thecvf.com/content_CVPR_2019/html/Orekondy_Knockoff_Nets_Stealing_Functionality_of_Black-Box_Models_CVPR_2019_paper.html PRADA: protecting against DNN model stealing attacks IEEE 2019 255 https://arxiv.org/abs/1805.02628 High accuracy and high fidelity extraction of neural networks, USENIX 2020 169 https://www.usenix.org/conference/usenixsecurity20/presentation/jagielski Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures USENIX 2020 159 https://www.usenix.org/conference/usenixsecurity20/presentation/yan CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel USENIX 2019 142 https://www.usenix.org/conference/usenixsecurity19/presentation/batina","link":"/2022/10/26/research-model-extraction/"}],"tags":[{"name":"Model Extraction","slug":"Model-Extraction","link":"/tags/Model-Extraction/"},{"name":"Model Inversion Attack","slug":"Model-Inversion-Attack","link":"/tags/Model-Inversion-Attack/"},{"name":"Membership Inference Attack","slug":"Membership-Inference-Attack","link":"/tags/Membership-Inference-Attack/"},{"name":"数据重建","slug":"数据重建","link":"/tags/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%BB%BA/"},{"name":"model extraction","slug":"model-extraction","link":"/tags/model-extraction/"}],"categories":[],"pages":[]}