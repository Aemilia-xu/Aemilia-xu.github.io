{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment aaaaa","link":"/2022/10/18/hello-world/"},{"title":"【论文笔记】Plug &amp; Play Attacks—— Towards Robust and Flexible Model Inversion Attacks","text":"https://zhuanlan.zhihu.com/p/536091331 摘要关于Model Inversion Attacks（模型反转攻击），即通过利用模型的learned knowledge，从目标分类器的私有训练数据中创造出反映class特征的合成图像。相当于从模型中提取出每种类别特定的特征。这也是很严重安全隐患，因为可能可以提取出如人脸、指纹、身份信息等敏感隐私信息，那么攻击者可以重构人脸，冒用身份…… 以前的研究都是训练GAN（生成对抗网络）来作为image priors（先验）。但是存在耗时、耗力、易受数据集分布变化的影响。本文提出的Plug &amp; Play攻击可以减少对image prior的依赖，只需要一个GAN网络即可对大范围的目标进行攻击。而且即使使用的是预训练好的GAN模型、数据分布发生很大变化也可以达到很好的攻击效果。 1 Introduction为了解决训练过程中会出现的梯度消失的问题，使用Poincare损失函数，而不是交叉熵损失函数。加入随机转化避免过拟合。首次提出了要从攻击结果中找出有意义的样本的重要性。 2 Model Inversion in Deep Learning 介绍了以前一些攻击方法，及其缺陷。 目前有三种攻击方式：optimization-based, training-based, or architecture-based optimization-based：通过产生合成的模型输入来找类型特征，也基于梯度下降的方法 training-based：训练GAN模型，将目标模型视作一个加密器，要找解密器 这些都不是为了获取隐私信息，只是探寻目标模型各个类别的特征 可能存在fooling image（无意义的噪点图像），out-of-distribution data（如猫狗分类中出现了一个熊猫图像） 影响因素：distributional shifts, vanishing gradients, and non-robust target models 3 Generative Model Inversion Attacks 定义了理想的MIA攻击，以及会影响MIA攻击效果的因素 假设：目标分类模型为，且为对x是否为类c∈C的预测分数，敌手可以访问目标模型（作为white box），且无限次地问询，但并不知道C的任何信息。 敌手要构造一个合成图像x^，满足时目标类别c的特征，泄露了隐私信息。这在较浅的神经网络中可行，但是现在流行的深层神经网络就不行了。 之后产生了使用GAN网络来生成样本（将特征向量z映射到图像空间），并训练鉴别器D来鉴别是否是真实样本x还是由G生成的G(z)，得到等式。这样或许可以得到评分（$y_c=M_{target}(\\overset{\\thicksim}{x} )_c$）最高的$ \\overset{\\thicksim}{x} = G(\\overset{\\thicksim}{z})$。但是这个构造得到的$\\overset{\\thicksim}{x}$不一定具有意义。 为图像的分布，令为X中的样本可能具有的人类可识别特征的分布，令为c类的特征。如再面部图像中，F和P的区别在于，F可能包含发色、皱纹、瞳孔间距等面部特征，而P还多了与身份信息无关的（如服装、背景等）。也就是说，$F$只包含和人物的身份信息有关的信息，而$P$还包含了无用的信息。 注意，不同类别之间的特征可能相同。 现假设，有2个类别满足，且，即特征不相同。 G可以拟合P(X)，进而近似F(X)，然后合成符合这个分布的样本x。 【问题】$\\overset{\\thicksim}{X}$是如何得到的 4 Towards Robust and Flexible MIAs 介绍本文提出的Plug &amp; Play攻击方式。这种方式鲁棒性更好，也更适用于distributional shift setting分布转换设置 首先，latent vectors潜伏向量被采样并映射到它们的中间表征w（用预训练的StyleGAN2模型） 然后根据w生成图像，进行转换（Transformations），并输入目标模型。 最后，在目标模型的输出和目标类别c上计算Poincare损失，并通过反向传播损失和执行梯度下降步骤更新w。优化完成后，根据其对随机变换的鲁棒性，选择一个结果子集。 4.1. Target-Independent Image Priors采用预训练的StyleGAN2模型作为图像的先验（不需要辅助输入和训练特定数据集的模型）。只需要是同一领域预训练好的模型。 这里预训练好的StyleGAN2包含2个模块：会将随机的latent vector潜伏向量（服从标准正态分布）的z映射到intermediate latent representation中间潜伏向量w。则根据w来生成图片 4.2. Increasing Robustness by Transformations进行一系列的图像变换，并令。在优化过程中，先进行这些可导的图像变换，再输入到目标模型中得到预测分数。即在前向传播过程中计算的是 这样，若这些变换后的图像越接近目标分布，攻击就会越成功。而且可以增加生成的图像的鲁棒性 4.3 Overcoming Vanishing Gradients以前的MIA都基于交叉熵损失函数，这样关于output logit的偏导为，容易发生梯度消失。而初始的潜伏向量又是随机产生的，随着调整，其预测分数会越来越高，导致梯度趋于消失。这样如果预先的采样不好的话，由于梯度下降的问题，对初始图像的改动就很小，攻击就会不成功。 为了解决这个问题，采用Poincaré distance作为损失函数。（其中$||·||_2$是欧几里得范数）。它是一个特殊hyperbolic space(双曲空间)中的距离度量。在这个空间里面，一个圆的面积随着半径的增加指数型增加。 【注】欧几里得范数即$L2$范数 4.4 Selecting Meaningful Attack Results 选用的方法是transformation-based selection，即通过变换，重新计算得分，选择最高的 攻击结果可能存在误导。本文采取的方法如下： 选取大量的符合正态分布的样本，并映射到intermediate latent space中间隐藏空间$W$ 为每一个w生成图像，并进行剪裁和调整大小的变换。计算平均预测分数，以及horizontally flipped counterpart with $M_{target}$，为每个类别c选择其中得分最高的初始图像。 根据蒙特卡罗方法 计算预期稳健预测分数，并进行$N=100$次随机的图像变换，选取其中得分最高的50个作为最终攻击结果。【注意】这里选取的变换要和优化过程中选取的不同或更强（否则poorly generated的样本可能会过拟合） 总结Plug &amp; Play攻击要解决的问题就是： 5 Experiments 介绍了验证Plug &amp; Play有效性的实验，和其他攻击方法的对比 评价标准有： 在目标模型的数据集上训练Inception-v3模型，然后用该模型对攻击结果进行打分，给出在目标类别上的top-1和top-5准确率。 计算平均特征距离$δ_{eval}$，针对面部图像用预训练的FaceNet来测量特征距离$δ_{face}$，越小说明越接近训练数据。 Fréchet inception distance（FID）","link":"/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02/"},{"title":"【论文笔记】Reconstructing Training Data with Informed Adversaries","text":"摘要&amp;介绍由于机器学习模型可以有“记忆”功能，那么当训练数据涉及隐私信息时，若被攻击者还原出原始训练数据，就会很危险。这个论文就是探讨这样的攻击是否可能。 所有的训练数据中，只有一个是未知的。目标就是还原出这一个未知的数据样本，找到攻击的方式、泄露多少信息可以达到被还原、哪些特性会导致可还原、自我检查模型的安全性（不泄露信息） 本文提出了一种研究ML模型重构攻击的可行性的通用方法，而无需假设模型的类型或访问中间梯度，并启动了一项能够防止这类攻击的缓解策略的研究 RecoNN一种用于重现数据而训练的神经网络 II. Reconstruction with Informed Adversaries该部分主要介绍了为什么定义了这样很强的infromed adversaries，即敌手知道除了target以外的所有数据。以及这样的reconstruction攻击和membership inference攻击、attribute inference攻击之间的关系。 III. Reconstruction in convex settingA. Reconstruction Strategy for Convex Models针对ERM（经验风险最小化问题）下凸监督学习模型的攻击，在没有side knowledge的时候，对线性回归、岭回归、Logistic回归都可以实现攻击。 若风险函数为，且，则最优解为 敌手有访问模型的white box（说明知道模型的结构、参数信息），并且有其他所有样本D_ 的信息，那么target z就满足一个方程式 监督学习中一个样本用(x,y)即d维的特征向量x和1维的标签y来表示，则要从d’个等式中解出d+1个未知数（p.s. d’是模型空间的维度） 【解读】为什么是d’个等式？因为模型空间中，每个模型都有一个方程 B. Closed-Form Reconstruction Against GLMs 证明在用截距项拟合的GLM（广义线性模型）的情况下，存在这种攻击的闭式解决方案。 这个攻击下，informed敌手不需要其他关于z的side knowledge 【证明】 IV. A General Reconstruction Attack 针对一般的机器学习(ML)模型的重建攻击 直观上，$z$对模型$θ=A(D_∪{z})$的影响和$\\overset{\\thicksim}{z}$对$\\overset{\\thicksim}{θ}=A(D_ ∪\\overset{\\thicksim}{z})$的影响是一样的，所以可以通过反复尝试来找到$z$ A. General Attack Strategy记$A_{D_}=A(D_ ∪{z})$，那么当$D_$ 给定时，A就是Z→Θ的函数。那么reconstruction攻击就是要解出$z=A_{D_}^{-1}(θ)$ 【注意】这里是一般的ML模型，所以不一定能够保证是convex的，而且模型训练过程中可能有随机性 因为假设了敌手特别强大，所以可以采用枚举的方式，找到最接近θ的论文中采用的机制是using “neural networks to attack neural networks”，（用魔法打败魔法hhhh），构建了RecoNN模型 B. Training Reconstructor Networks本文中重构z的步骤如下 $\\bar D=\\{\\bar z_1,\\bar z_2,…,\\bar z_k\\}$是$Z$中的shadow targets，这属于关于$z$的side knowledge 枚举$\\bar D$中的所有元素，得到$\\bar θ_i$，从而得到了attack training data攻击训练数据 使用训练算法R得到RecoNN模型$Φ$ 应用$Φ$，得到结果$\\bar z$ 此外，假设了$X$和$Y$都是有限的，且$y$可以从$x$中直接推断出，那么只要重构样本的特征向量$x$即可 具体的训练方式见后面的讨论 using “neural networks to attack neural networks”的相关工作被应用于了membership inference，model inversion模型反演，property inference V. Experimental SetupA. Default Settings实验中使用的模型的超参数 a. 数据集的分割：$D_$、D^和测试目标集是三个不相交的集合 b. 生成模型的训练：训练方式采用的就是标准的梯度下降法，且使用了full batches。假设了敌手直到模型的初始状态，所有模型的初始状态都一致（之后也有讨论使用mini-batching和随机初始化情况下的攻击） MINST数据集上训练出来的模型准确率更加高（因为CIFAR-10更加复杂），所以对CIFAR-10的攻击可能更加困难，需要更加庞大的shadow points去训练RecoNN c. 重建模型的训练：When training the reconstructor, shadow model parameters across layers are flattened and concatenated together.？？？什么意思. 在训练的时候，跨层的参数被扁平化，且连接在一起。还需要把所有的坐标点都缩放到0均值和平均方差，因为有些参数很小。 针对MINST，采用MAE（平均绝对误差）+MSE（均方误差）的方式来计算shadow targets和reconstructor outputs之间的loss。 针对CIFAR-10，增加了LPIPS loss（感知损失）和GAN-like Discriminator loss，都是为了提高图像的视觉质量 B. Criteria for Attack Success介绍了实验中设置的评估指标 a. MSE：均方差 b. LPIPS：感知损失，即比较深度特征，更加接近人类视觉 c. KL散度： d. Nearest Neighbor Oracle: 一个oracle，可以猜测离z最接近的点z^ VI. Empirical Studies in reconstruction总结采用的策略是针对已知的所有训练数据和shadow targets训练得到的所有模型θ~i~来训练RecoNN模型Φ，来得到θ对应的样本z，即using “neural networks to attack neural networks” 相关知识 membership inference attacks (MIA)：成员推理攻击，即判断一个对象是否是模型训练时使用的数据https://blog.csdn.net/xff1994/article/details/89964552 attribute inference attacks (AIA) ：推理攻击 property inference attacks (PIA)：属性推理攻击，AIA的一种泛化攻击，一般都是得到样本数据的一些属性、平均信息，不会损害个人隐私 GLM广义线性模型：https://zhuanlan.zhihu.com/p/22876460 https://xg1990.com/blog/archives/304 例如线性回归中假设y|x服从正态分布，Logistic回归中假设y|x服从二项分布 ![](论文笔记1/image-20221008144438312.png) canonical exponential family 指数分布族 而在GLM中，假设y|x满足这种指数分布族是因为这样下的y|x的期望、方差都非常简单，而要求的目标函数h(x;θ)就是y|x的期望，这样就很简单了。 canonical link function 正则关联函数，是canonical response function（正则响应函数）的倒数 LPIPS loss（感知损失）：用深度特征来度量图像的相似度 https://zhuanlan.zhihu.com/p/206470186 GAN-like Discriminator loss GAN网络（生成对抗网络是要通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布） 问题 没明白”When training the reconstructor, shadow model parameters across layers are flattened and concatenated together.“ 扁平化就是说降低维度吧。","link":"/2022/10/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01/"}],"tags":[{"name":"Model Inversion Attack","slug":"Model-Inversion-Attack","link":"/tags/Model-Inversion-Attack/"},{"name":"数据重建","slug":"数据重建","link":"/tags/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%BB%BA/"}],"categories":[],"pages":[]}